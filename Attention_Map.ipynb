{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.load('./log/Audio_Aware/loss.pt')\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_loss = [l.item() for l in loss[\"dev_loss\"]]\n",
    "dev_loss\n",
    "\n",
    "plt.plot(range(1,11), dev_loss, 'go')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_loss = [l.item() for l in loss[\"dev_loss\"]]\n",
    "dev_loss\n",
    "\n",
    "plt.plot(range(1,11), dev_loss, 'go')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = [200 * k for k in range(1, 25)] + [5005]\n",
    "step = np.array(step)\n",
    "for i in range(2, 10):\n",
    "    step = np.concatenate((step, single_step * i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = [l.item() for l in loss[\"training_loss\"]]\n",
    "train_loss\n",
    "\n",
    "plt.plot(step, train_loss, 'go')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RescoreBert(\n",
    "    train_batch=1,\n",
    "    test_batch=1,\n",
    "    nBest=10,\n",
    "    use_MWER=True,\n",
    "    use_MWED=False,\n",
    "    device=device,\n",
    "    lr=1e-4,\n",
    "    weight=0.59\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('./checkpoint/MWER/checkpoint_train_4.pt')\n",
    "model.model.load_state_dict(state_dict[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token = '甚 至 出 现 交 易 几 乎 停 止 的 情 况'.split()\n",
    "# token = '甚 至 出 现 交 易 几 乎 停 滞 的 情 况'.split()\n",
    "token_ref = '楼 市 调 控 的 行 政 手 段 宜 减 不 宜 加'.split()\n",
    "token_hyp = '楼 市 调 控 的 行 政 手 段 意 见 不 一 一'.split()\n",
    "token_hyp_2 = '楼 市 调 控 的 行 政 手 段 意 见 不 一'.split()\n",
    "token_id_ref = tokenizer.convert_tokens_to_ids(token_ref)\n",
    "token_id_hyp = tokenizer.convert_tokens_to_ids(token_hyp)\n",
    "token_id_hyp_2 = tokenizer.convert_tokens_to_ids(token_hyp_2)\n",
    "token_id_ref = [101] + token_id_ref + [102]\n",
    "token_id_hyp = [101] + token_id_hyp + [102]\n",
    "token_id_hyp_2 = [101] + token_id_hyp_2 + [102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = torch.nn.CosineSimilarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_id_ref = torch.tensor(token_id_ref).unsqueeze(0).to(device)\n",
    "input_id_hyp = torch.tensor(token_id_hyp).unsqueeze(0).to(device)\n",
    "input_id_hyp_2 = torch.tensor(token_id_hyp_2).unsqueeze(0).to(device)\n",
    "\n",
    "print(input_id_hyp)\n",
    "print(input_id_hyp_2)\n",
    "with torch.no_grad():\n",
    "    output_ref = model.model(\n",
    "        input_ids = input_id_ref,\n",
    "    )[0]\n",
    "    output_hyp = model.model(\n",
    "        input_ids = input_id_hyp,\n",
    "    )[0]\n",
    "\n",
    "    output_hyp_2 = model.model(\n",
    "        input_ids = input_id_hyp_2,\n",
    "    )[0]\n",
    "\n",
    "    sim = cos(output_ref.squeeze(0)[0], output_hyp_2.squeeze(0)[0])\n",
    "    print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_dict = dict()\n",
    "recog_set = ['dev', 'test']\n",
    "for task in recog_set:\n",
    "    distribution_dict[task] = {\n",
    "        1:0,\n",
    "        0.95:0,\n",
    "        0.75:0,\n",
    "        0.5:0,\n",
    "        0.25:0,\n",
    "        0:0,\n",
    "        'same':0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = torch.nn.CosineSimilarity(0)\n",
    "for task in recog_set:\n",
    "    print(task)\n",
    "    data = None\n",
    "    with open(f'./data/aishell_{task}/rescore/MD_rescore_data.json') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    for k in data['utts'].keys():\n",
    "        \n",
    "        token_ref = data['utts'][k]['output']['text_token'].split()\n",
    "        token_hyp = data['utts'][k]['output']['rec_token'].split()  \n",
    "\n",
    "        if (token_ref == token_hyp):\n",
    "            distribution_dict[task]['same'] += 1\n",
    "            continue\n",
    "\n",
    "        token_id_ref = tokenizer.convert_tokens_to_ids(token_ref)\n",
    "        token_id_hyp = tokenizer.convert_tokens_to_ids(token_hyp)\n",
    "        token_id_ref = [101] + token_id_ref + [102]\n",
    "        token_id_hyp = [101] + token_id_hyp + [102]\n",
    "\n",
    "        input_id_ref = torch.tensor(token_id_ref).unsqueeze(0).to(device)\n",
    "        input_id_hyp = torch.tensor(token_id_hyp).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            output_ref = model.model(\n",
    "                input_ids = input_id_ref,\n",
    "            )[0]\n",
    "\n",
    "            output_hyp = model.model(\n",
    "                input_ids = input_id_hyp,\n",
    "            )[0]\n",
    "\n",
    "        output_ref = output_ref.squeeze(0)[0]\n",
    "        output_hyp = output_hyp.squeeze(0)[0]\n",
    "\n",
    "        sim = cos(output_ref, output_hyp)\n",
    "        if (sim >= 1):\n",
    "            distribution_dict[task][1] += 1\n",
    "        elif (sim < 1 and sim >= 0.95):\n",
    "            distribution_dict[task][0.95] += 1\n",
    "        elif (sim < 0.95 and sim >= 0.75):\n",
    "            distribution_dict[task][0.75] += 1\n",
    "        elif (sim < 0.75 and sim >= 0.5):\n",
    "            distribution_dict[task][0.5] += 1\n",
    "        elif (sim < 0.5 and sim >= 0.25):\n",
    "            distribution_dict[task][0.25] += 1\n",
    "        elif (sim < 0.25):\n",
    "            distribution_dict[task][0] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_dict['dev']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_dict['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'test'\n",
    "labels = [\"Sim == 1\", \"0.95 <= Sim < 1\", \"0.75 <= Sim < 0.95\", \"0.5 <= Sim < 0.75\", \"0.25 <= Sim < 0.5\", \"Sim < 0.25\", \"Same\"]\n",
    "value = list(distribution_dict[task].values())\n",
    "fig, axe = plt.subplots()\n",
    "patches, texts = plt.pie(\n",
    "    value,\n",
    "    startangle=90, radius=1.2 \n",
    "    )\n",
    "\n",
    "legend_labels = ['{} : {}'.format(k, v) for k, v in zip(labels, value)]\n",
    "\n",
    "# sort_legend = True\n",
    "# if sort_legend:\n",
    "#     patches, labels, dummy =  zip(*sorted(zip(patches, labels, list(distribution_dict['test'].values())),\n",
    "#                                           key=lambda x: x[2],\n",
    "#                                           reverse=True))\n",
    "\n",
    "plt.legend(patches, legend_labels, loc = 'best', bbox_to_anchor=(-0.1, 1), fontsize=8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "input_id = torch.tensor(token_id_hyp).unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    attention_map = model.model(\n",
    "        input_ids = input_id,\n",
    "        output_attentions = True\n",
    "    ).attentions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_id_ref = torch.tensor(token_id_ref).unsqueeze(0)\n",
    "input_id_hyp = torch.tensor(token_id_hyp).unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    output_ref = model.model(\n",
    "        input_ids = input_id_ref,\n",
    "    )[0]\n",
    "\n",
    "    output_hyp = model.model(\n",
    "        input_ids = input_id_hyp,\n",
    "    )[0]\n",
    "\n",
    "output_ref = output_ref.squeeze(0)[0]\n",
    "output_hyp = output_hyp.squeeze(0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(cos(output_ref.squeeze(0)[0], output_hyp.squeeze(0)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axe = plt.subplots(figsize = (10,10))\n",
    "total_map = np.zeros(attention_map[0][0][0].shape)\n",
    "for i in range(len(attention_map)):\n",
    "    np_map = attention_map[i].numpy().squeeze(0)\n",
    "    np_map = np_map[-1]\n",
    "    np_map = np_map * 1e8\n",
    "    np_map = np_map.astype(int)\n",
    "    np_map = np_map / np.linalg.norm(np_map)\n",
    "    total_map += np_map\n",
    "# total_map = np.insert(total_map, 10, 0, axis = 1)\n",
    "# total_map = np.insert(total_map, 10, 0, axis = 0)\n",
    "c = axe.pcolor(total_map, vmin = 0, vmax = 1)\n",
    "fig.colorbar(c, ax = axe)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b61d4f77dfbe8390dfee6517b8c5af4fcaae1c92a3d097799b61ff8897a7412"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
