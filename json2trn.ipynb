{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [],
         "source": [
            "import json"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [],
         "source": [
            "data_name = 'csj'\n",
            "recog_set = ['dev', 'eval1', 'eval2', 'eval3']\n",
            "setting = ['withLM']\n",
            "# setting = ['noLM']\n",
            "# setting = ['withLM', 'noLM']\n",
            "nbest = 50"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "for s in setting:\n",
            "    for r in recog_set:\n",
            "        print(r)\n",
            "        json_name = f'./data/{dataset}/{s}/{r}_data.json'\n",
            "        with open(json_name, 'r') as f,\\\n",
            "            open(f'./data/{dataset}/{s}/{r}_hyp.trn', 'w') as hyp,\\\n",
            "            open(f'./data/{dataset}/{s}/{r}_ref.trn', 'w') as ref:\n",
            "            j = json.load(f)\n",
            "            for k in j['utts'].keys():\n",
            "                assert(len(j['utts'][k]['output']) == nbest)\n",
            "                for i, h in enumerate(j['utts'][k]['output']):\n",
            "                    # print(j['utts'][k]['output'])\n",
            "                    seq = [t for t in h['rec_token'].split()]\n",
            "                    hyp_seq = \" \".join(seq[:-1])\n",
            "                    hyp.write( f'{hyp_seq} ({j[\"utts\"][k][\"utt2spk\"].replace(\"-\", \"_\")}-{k}-hyp[{i}])\\n' )\n",
            "                    # hyp.write(hyp_seq + '(' + j[\"utts\"][k][\"utt2spk\"].replace(\"-\", \"_\") + '-' + 'hyp' + i + ')')\n",
            "\n",
            "                    ref_seq = h[\"token\"]\n",
            "                    ref.write( f'{ref_seq} ({j[\"utts\"][k][\"utt2spk\"].replace(\"-\", \"_\")}-{k}-hyp[{i}])\\n' )"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "上面code跑完 執行espnet的\n",
            "```sclite -r ./ref.trn -h  ./hyp.trn -i rm -o all stdout >  ./result.txt```"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "recog_set = ['dev', 'test']"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "noLM:dev\n",
                  "4000\n",
                  "noLM:eval1\n",
                  "1272\n",
                  "noLM:eval2\n",
                  "1292\n",
                  "noLM:eval3\n",
                  "1385\n"
               ]
            }
         ],
         "source": [
            "for s in setting:\n",
            "    for task in recog_set:\n",
            "        nbest_err = []\n",
            "        print(f'{s}:{task}')\n",
            "        with open(f'./data/{data_name}/{s}/{task}_result.txt', 'r') as f:\n",
            "            temp_err = []\n",
            "            for i, line in enumerate(f):\n",
            "                if ('Scores' in line):\n",
            "                    scores = line.split()[-4:]\n",
            "                    temp_err.append(scores)\n",
            "                    if (len(temp_err) == nbest):\n",
            "                        nbest_err.append(temp_err)\n",
            "                        temp_err = []\n",
            "            names = []\n",
            "            nbest_token = []\n",
            "            nbest_score = []\n",
            "            ref_text = []\n",
            "            with open(f'./data/{data_name}/{s}/{task}_data.json', 'r') as f:\n",
            "                j = json.load(f)\n",
            "                for k in j['utts'].keys():\n",
            "                    token = []\n",
            "                    score = []\n",
            "                    for i, h in enumerate(j['utts'][k]['output']):\n",
            "                        token.append(h[\"rec_text\"])\n",
            "                        score.append(h['score'])\n",
            "                    nbest_token.append(token)\n",
            "                    nbest_score.append(score)\n",
            "                    ref_text.append(j['utts'][k]['output'][0]['text'])\n",
            "                    names.append(k)\n",
            "\n",
            "            dataset = []\n",
            "            for name, token, score, err, ref in zip(names, nbest_token, nbest_score, nbest_err, ref_text):\n",
            "                temp_dict = dict()\n",
            "                temp_dict['name'] = name\n",
            "                temp_dict['token'] = [t[:-5]  for  t  in token]\n",
            "                temp_dict['score'] = score\n",
            "                temp_dict['err'] =  [[int(s) for s in sc] for sc in err]\n",
            "                temp_dict['ref'] =  ref \n",
            "                dataset.append(temp_dict)\n",
            "            print(len(dataset))\n",
            "        with open(f\"./data/{data_name}/{task}/data_{s}.json\", 'w') as f:\n",
            "            json.dump(dataset, f, ensure_ascii=False, indent = 4)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": []
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Result trn"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [],
         "source": [
            "dataname = 'aishell'\n",
            "name = \"Audio_Aware\"\n",
            "setting = 'noLM'\n",
            "recog_set = ['dev', 'test']\n",
            "nbest = 4\n",
            "training = 'MD'"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# BART"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [],
         "source": [
            "for r in recog_set:\n",
            "    json_name = f\"./src/Correction/data/{dataname}/{setting}/{r}/{nbest}best_rescore_data.json\"\n",
            "    with open(json_name, 'r') as f, \\\n",
            "            open(f'./src/Correction/data/{dataname}/{setting}/{r}/hyp.trn', 'w') as hyp, \\\n",
            "            open(f'./src/Correction/data/{dataname}/{setting}/{r}/ref.trn', 'w') as ref:\n",
            "        j = json.load(f)\n",
            "        for k in j['utts'].keys():\n",
            "                # print(j['utts'][k]['output'])\n",
            "            hyp_seq = j['utts'][k]['output']['recog_text']\n",
            "            hyp.write( f'{hyp_seq} ({k})\\n' )\n",
            "            # hyp.write(hyp_seq + '(' + j[\"utts\"][k][\"utt2spk\"].replace(\"-\", \"_\") + '-' + 'hyp' + i + ')')\n",
            "            ref_seq = j['utts'][k]['output']['ref_token']\n",
            "            ref.write( f'{ref_seq} ({k})\\n' )"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [],
         "source": [
            "for r in recog_set:\n",
            "    json_name = f\"./src/Correction/data/{dataname}/{setting}/{r}/{nbest}align/correct_data.json\"\n",
            "    with open(json_name, 'r') as f, \\\n",
            "            open(f'./src/Correction/data/{dataname}/{setting}/{r}/{nbest}align/hyp.trn', 'w') as hyp, \\\n",
            "            open(f'./src/Correction/data/{dataname}/{setting}/{r}/{nbest}align/ref.trn', 'w') as ref:\n",
            "        j = json.load(f)\n",
            "        for k in j['utts'].keys():\n",
            "                # print(j['utts'][k]['output'])\n",
            "            hyp_seq = j['utts'][k]['hyp']\n",
            "            hyp.write( f'{hyp_seq} ({k})\\n' )\n",
            "            # hyp.write(hyp_seq + '(' + j[\"utts\"][k][\"utt2spk\"].replace(\"-\", \"_\") + '-' + 'hyp' + i + ')')\n",
            "            ref_seq = j['utts'][k]['ref']\n",
            "            ref.write( f'{ref_seq} ({k})\\n' )"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# RescoreBert"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "sclite -r ./50best_hyp.trn -h  ./50best_ref.trn -i rm -o all stdout >  ./50best_result.txt\n",
                  "sclite -r ./50best_hyp.trn -h  ./50best_ref.trn -i rm -o all stdout >  ./50best_result.txt\n"
               ]
            }
         ],
         "source": [
            "for task in recog_set:\n",
            "    json_name = f\"./data/{dataname}/{task}/{training}/{setting}/{nbest}best_rescore_data.json\"\n",
            "    with open(json_name, 'r') as f, \\\n",
            "        open(f'./data/{dataname}/{task}/{training}/{setting}/{nbest}best_hyp.trn', 'w') as hyp, \\\n",
            "        open(f'./data/{dataname}/{task}/{training}/{setting}/{nbest}best_ref.trn', 'w') as ref:\n",
            "        j = json.load(f)\n",
            "        for k in j['utts'].keys():\n",
            "                # print(j['utts'][k]['output'])\n",
            "            hyp_seq = j['utts'][k]['output']['rec_token']\n",
            "            hyp.write( f'{hyp_seq} ({k})\\n' )\n",
            "            # hyp.write(hyp_seq + '(' + j[\"utts\"][k][\"utt2spk\"].replace(\"-\", \"_\") + '-' + 'hyp' + i + ')')\n",
            "            ref_seq = j['utts'][k]['output']['text_token']\n",
            "            ref.write( f'{ref_seq} ({k})\\n' )\n",
            "    print(f'sclite -r ./{nbest}best_hyp.trn -h  ./{nbest}best_ref.trn -i rm -o all stdout >  ./{nbest}best_result.txt')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "for r in recog_set:\n",
            "    json_name = f'./data/{dataset}_{r}/{name}/rescore_data.json'\n",
            "    with open(json_name, 'r') as f, open(f'./data/{dataset}_{r}/{name}/hyp_rescore.trn', 'w') as hyp, open(f'./data/{dataset}_{r}/{name}/ref_rescore.trn', 'w') as ref:\n",
            "        j = json.load(f)\n",
            "        for k in j['utts'].keys():\n",
            "                # print(j['utts'][k]['output'])\n",
            "            hyp_seq = j['utts'][k]['rec_text']\n",
            "            hyp.write( f'{hyp_seq} ({k})\\n' )\n",
            "            # hyp.write(hyp_seq + '(' + j[\"utts\"][k][\"utt2spk\"].replace(\"-\", \"_\") + '-' + 'hyp' + i + ')')\n",
            "            ref_seq = j['utts'][k]['ref_text']\n",
            "            ref.write( f'{ref_seq} ({k})\\n' )"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from transformers import BertTokenizer\n",
            "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import json\n",
            "import torch\n",
            "# for r in recog_set:\n",
            "best_weight = 0\n",
            "best_cer = 100\n",
            "with open(f\"./data/{dataname}/dev/{nbest}best_{training}_rescore_data_{setting}.json\") as f:\n",
            "    data = json.load(f)\n",
            "    \n",
            "    for n in range(101):\n",
            "        weight = n * 0.01\n",
            "        c = 0\n",
            "        s = 0\n",
            "        de = 0\n",
            "        i = 0\n",
            "        for d in data:\n",
            "            score = torch.tensor(d['score'])\n",
            "            pll = torch.tensor(d['pll'])\n",
            "            cer = d['err']\n",
            "                \n",
            "            result = score + weight * pll\n",
            "\n",
            "            max_index = torch.argmax(result).item()\n",
            "\n",
            "            c += cer[max_index][0]\n",
            "            s += cer[max_index][1]\n",
            "            de += cer[max_index][2]\n",
            "            i += cer[max_index][3]\n",
            " \n",
            "        cer = (s + de + i) / (c + s + de)\n",
            "\n",
            "        if (cer < best_cer):\n",
            "            best_cer = cer\n",
            "            best_weight = weight\n",
            "\n",
            "print(f'best weight:{best_weight} with cer:{best_cer}')\n",
            "\n",
            "for task in recog_set:\n",
            "    with open(f\"./data/{dataname}/{task}/{nbest}best_{training}_rescore_data_{setting}.json\", 'r') as f, \\\n",
            "         open(f'./data/{dataname}/{task}/hyp_mlm.trn', 'w') as h,\\\n",
            "         open(f'./data/{dataname}/{task}/ref_mlm.trn', 'w') as g:\n",
            "        data = json.load(f)\n",
            "        for n, d in enumerate(data):\n",
            "            score = torch.tensor(d['score'])\n",
            "            pll = torch.tensor(d['pll'])\n",
            "            cer = d['err']\n",
            "            ref = d['ref'][5:-5]\n",
            "            token = d['token']\n",
            "                \n",
            "            result = score + weight * pll\n",
            "\n",
            "            max_index = torch.argmax(result).item()\n",
            "\n",
            "            best_hyp = token[max_index]\n",
            "            sep = best_hyp.index(102)\n",
            "            hyp_seq = tokenizer.convert_ids_to_tokens(best_hyp[1:sep])\n",
            "\n",
            "            h.write( f'{\" \".join(hyp_seq)} ({r}_{n})\\n' )\n",
            "            g.write( f'{\" \".join(list(ref))} ({r}_{n})\\n')\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3.9.5 (conda)",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.9.5"
      },
      "orig_nbformat": 4,
      "vscode": {
         "interpreter": {
            "hash": "a7ccf5fb952618089b6fc9511fb1d1e0b820b57f976eb5c331d871bc3f52c234"
         }
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
