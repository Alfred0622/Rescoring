{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from nBestAligner.nBestAlign import align\n",
            "import torch"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "nbest = [\n",
            "    [1,2,3,4,6],\n",
            "    [0, 1, 2, 10],\n",
            "    [2,3,5,7],\n",
            "    [5,10, 4, 8],\n",
            "]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "device = 'cuda' if  torch.cuda.is_available() else 'cpu'"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "device = torch.device(device)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "align(nbest, 4)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from sentence_transformers import SentenceTransformer"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "model = SentenceTransformer(\n",
            "                \"cyclone/simcse-chinese-roberta-wwm-ext\",\n",
            "                device = device\n",
            "            )\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "tokens = ['广州市房地产中介协会分析'].to(device)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "embedding = model.encode(tokens)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "embedding.shape"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from transformers import BertTokenizer\n",
            "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "token = tokenizer.convert_tokens_to_ids(list('广州市房地产中介协会分析'))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "model.encode(token)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [],
         "source": [
            "import torch\n",
            "import logging\n",
            "from torch.nn.functional import log_softmax\n",
            "from transformers import (\n",
            "    BertForMaskedLM,\n",
            "    AutoModelForCausalLM,\n",
            "    BertTokenizerFast,\n",
            ")\n",
            "from torch.optim import AdamW"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [],
         "source": [
            "model = AutoModelForCausalLM.from_pretrained('ckiplab/gpt2-base-chinese')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [],
         "source": [
            "seq = torch.tensor([\n",
            "    [\n",
            "                101,\n",
            "                2408,\n",
            "                2336,\n",
            "                2356,\n",
            "                2791,\n",
            "                1765,\n",
            "                772,\n",
            "                704,\n",
            "                792,\n",
            "                1291,\n",
            "                833,\n",
            "                1146,\n",
            "                3358,\n",
            "                102\n",
            "            ],\n",
            "            [\n",
            "                101,\n",
            "                2408,\n",
            "                2336,\n",
            "                2356,\n",
            "                2791,\n",
            "                1765,\n",
            "                772,\n",
            "                704,\n",
            "                6381,\n",
            "                1291,\n",
            "                833,\n",
            "                102,\n",
            "                0,\n",
            "                0\n",
            "            ]\n",
            "        ])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [],
         "source": [
            "output = model(seq)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "torch.Size([2, 14, 21128])"
                  ]
               },
               "execution_count": 5,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "output.logits.shape"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [],
         "source": [
            "non_cls_index = (seq != 101)\n",
            "non_sep_index = (seq != 102)\n",
            "non_pad_index = (seq != 0)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "torch.Size([2, 14])"
                  ]
               },
               "execution_count": 8,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "temp_logit = torch.logical_and(non_cls_index, non_sep_index)\n",
            "token_index = torch.logical_and(temp_logit, non_pad_index)\n",
            "token_index.shape"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [],
         "source": [
            "real_token = seq[token_index]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 21,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "tensor([2408, 2336, 2356, 2791, 1765,  772,  704,  792, 1291,  833, 1146, 3358,\n",
                     "        2408, 2336, 2356, 2791, 1765,  772,  704, 6381, 1291,  833])"
                  ]
               },
               "execution_count": 21,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "seq[token_index]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "metadata": {},
         "outputs": [],
         "source": [
            "logit = output.logits"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "torch.Size([2, 14, 21128])"
                  ]
               },
               "execution_count": 11,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "logit.shape"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 12,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "torch.Size([2, 14])"
                  ]
               },
               "execution_count": 12,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "seq.shape"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 14,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "torch.Size([2, 14])"
                  ]
               },
               "execution_count": 14,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "score = logit.gather(2, seq.unsqueeze(2)).squeeze(-1)\n",
            "score.shape"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 26,
         "metadata": {},
         "outputs": [],
         "source": [
            "true_index = torch.nonzero(token_index)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 27,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "tensor(0)\n",
                  "tensor(0)\n",
                  "tensor(0)\n",
                  "tensor(0)\n",
                  "tensor(0)\n",
                  "tensor(0)\n",
                  "tensor(0)\n",
                  "tensor(0)\n",
                  "tensor(0)\n",
                  "tensor(0)\n",
                  "tensor(0)\n",
                  "tensor(0)\n",
                  "flush\n",
                  "tensor(1)\n",
                  "tensor(1)\n",
                  "tensor(1)\n",
                  "tensor(1)\n",
                  "tensor(1)\n",
                  "tensor(1)\n",
                  "tensor(1)\n",
                  "tensor(1)\n",
                  "tensor(1)\n",
                  "tensor(1)\n"
               ]
            }
         ],
         "source": [
            "sum_score = 0\n",
            "last_i = -1\n",
            "result = []\n",
            "for i, j in true_index:\n",
            "    if (last_i >= 0 and last_i != i):\n",
            "        print('flush')\n",
            "        result.append(sum_score)\n",
            "        sum_score = 0\n",
            "    print(i)\n",
            "    sum_score += score[i][j].item()\n",
            "    last_i = i.item()\n",
            "result.append(sum_score)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 139,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "[2.821585178375244, -20.840754985809326]"
                  ]
               },
               "execution_count": 139,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "result"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 36,
         "metadata": {},
         "outputs": [],
         "source": [
            "exp_logit = torch.exp(logit)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 38,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "tensor([-127098.0000, -141032.8750, -140960.9844, -145216.1875, -131136.9375,\n",
                     "        -124833.8438, -140478.4844, -138558.0000, -148117.0469, -142772.9219,\n",
                     "        -141909.0938, -144588.3281, -147351.7812, -148185.6562],\n",
                     "       grad_fn=<SumBackward1>)"
                  ]
               },
               "execution_count": 38,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "torch.sum(logit, -1)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "string = \"你好啊\"\n",
            "list(string)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [],
         "source": [
            "from transformers import BertTokenizer"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
                  "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
                  "The class this function is called from is 'BertTokenizer'.\n"
               ]
            }
         ],
         "source": [
            "tokenizer = BertTokenizer.from_pretrained('fnlp/bart-base-chinese')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [],
         "source": [
            "string1 = '我要吃飯要吃飯吃飯飯'\n",
            "string2 = '我要吃飯#要吃飯#吃飯#飯'"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "metadata": {},
         "outputs": [],
         "source": [
            "str_list1 = [x for x in string1]\n",
            "str_list2 = [x for x in string2]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "['[PAD]']"
                  ]
               },
               "execution_count": 9,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "tokenizer.convert_ids_to_tokens([0])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 19,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "['我', '要', '吃', '飯', '要', '吃', '飯', '吃', '飯', '飯']\n",
                  "['我', '要', '吃', '飯', '#', '要', '吃', '飯', '#', '吃', '飯', '#', '飯']\n"
               ]
            }
         ],
         "source": [
            "print(str_list1)\n",
            "print(str_list2)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 18,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "['我', '要', '吃', '飯', '要', '吃', '飯', '吃', '飯', '飯']\n",
                  "['我', '要', '吃', '飯', '#', '要', '吃', '飯', '#', '吃', '飯', '#', '飯']\n"
               ]
            }
         ],
         "source": [
            "print(tokenizer.tokenize(string1))\n",
            "print(tokenizer.tokenize(string2))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 21,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "True"
                  ]
               },
               "execution_count": 21,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "str_list2 == tokenizer.tokenize(string2)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 14,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "[2769, 6206, 1391, 7613, 108, 6206, 1391, 7613, 108, 1391, 7613, 108, 7613]"
                  ]
               },
               "execution_count": 14,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "tokenizer.convert_tokens_to_ids(str_list2)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "string = \"00010002\"\n",
            "string = string.lstrip('0')\n",
            "print(string)\n",
            "split_string = [string[max(i - 4, 0) : i] for i in range(len(string), 0 , -4)]\n",
            "split_string[::-1]"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "espnet",
         "language": "python",
         "name": "espnet"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.8.0"
      },
      "orig_nbformat": 4,
      "vscode": {
         "interpreter": {
            "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
         }
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
