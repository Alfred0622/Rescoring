{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [],
         "source": [
            "import torch"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "nbest = [\n",
            "    [1,2,3,4,6],\n",
            "    [0, 1, 2, 10],\n",
            "    [2,3,5,7],\n",
            "    [5,10, 4, 8],\n",
            "]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "device = 'cuda' if  torch.cuda.is_available() else 'cpu'"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "device = torch.device(device)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "align(nbest, 4)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from sentence_transformers import SentenceTransformer"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "model = SentenceTransformer(\n",
            "                \"cyclone/simcse-chinese-roberta-wwm-ext\",\n",
            "                device = device\n",
            "            )\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "tokens = ['广州市房地产中介协会分析'].to(device)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "embedding = model.encode(tokens)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "embedding.shape"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from transformers import BertTokenizer\n",
            "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "token = tokenizer.convert_tokens_to_ids(list('广州市房地产中介协会分析'))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "model.encode(token)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [],
         "source": [
            "import torch\n",
            "import logging\n",
            "from torch.nn.functional import log_softmax\n",
            "from transformers import (\n",
            "    BertForMaskedLM,\n",
            "    AutoModelForCausalLM,\n",
            "    BertTokenizerFast,\n",
            ")\n",
            "from torch.optim import AdamW"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [],
         "source": [
            "model = AutoModelForCausalLM.from_pretrained('ckiplab/gpt2-base-chinese')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [],
         "source": [
            "seq = torch.tensor([\n",
            "    [\n",
            "                101,\n",
            "                2408,\n",
            "                2336,\n",
            "                2356,\n",
            "                2791,\n",
            "                1765,\n",
            "                772,\n",
            "                704,\n",
            "                792,\n",
            "                1291,\n",
            "                833,\n",
            "                1146,\n",
            "                3358,\n",
            "                102\n",
            "            ],\n",
            "            [\n",
            "                101,\n",
            "                2408,\n",
            "                2336,\n",
            "                2356,\n",
            "                2791,\n",
            "                1765,\n",
            "                772,\n",
            "                704,\n",
            "                6381,\n",
            "                1291,\n",
            "                833,\n",
            "                102,\n",
            "                0,\n",
            "                0\n",
            "            ]\n",
            "        ])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [],
         "source": [
            "output = model(seq)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "torch.Size([2, 14, 21128])"
                  ]
               },
               "execution_count": 5,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "output.logits.shape"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [],
         "source": [
            "non_cls_index = (seq != 101)\n",
            "non_sep_index = (seq != 102)\n",
            "non_pad_index = (seq != 0)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "torch.Size([2, 14])"
                  ]
               },
               "execution_count": 8,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "temp_logit = torch.logical_and(non_cls_index, non_sep_index)\n",
            "token_index = torch.logical_and(temp_logit, non_pad_index)\n",
            "token_index.shape"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [],
         "source": [
            "real_token = seq[token_index]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 21,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "tensor([2408, 2336, 2356, 2791, 1765,  772,  704,  792, 1291,  833, 1146, 3358,\n",
                     "        2408, 2336, 2356, 2791, 1765,  772,  704, 6381, 1291,  833])"
                  ]
               },
               "execution_count": 21,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "seq[token_index]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "metadata": {},
         "outputs": [],
         "source": [
            "logit = output.logits"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "torch.Size([2, 14, 21128])"
                  ]
               },
               "execution_count": 11,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "logit.shape"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 12,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "torch.Size([2, 14])"
                  ]
               },
               "execution_count": 12,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "seq.shape"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 14,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "torch.Size([2, 14])"
                  ]
               },
               "execution_count": 14,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "score = logit.gather(2, seq.unsqueeze(2)).squeeze(-1)\n",
            "score.shape"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 26,
         "metadata": {},
         "outputs": [],
         "source": [
            "true_index = torch.nonzero(token_index)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 27,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "tensor(0)\n",
                  "tensor(0)\n",
                  "tensor(0)\n",
                  "tensor(0)\n",
                  "tensor(0)\n",
                  "tensor(0)\n",
                  "tensor(0)\n",
                  "tensor(0)\n",
                  "tensor(0)\n",
                  "tensor(0)\n",
                  "tensor(0)\n",
                  "tensor(0)\n",
                  "flush\n",
                  "tensor(1)\n",
                  "tensor(1)\n",
                  "tensor(1)\n",
                  "tensor(1)\n",
                  "tensor(1)\n",
                  "tensor(1)\n",
                  "tensor(1)\n",
                  "tensor(1)\n",
                  "tensor(1)\n",
                  "tensor(1)\n"
               ]
            }
         ],
         "source": [
            "sum_score = 0\n",
            "last_i = -1\n",
            "result = []\n",
            "for i, j in true_index:\n",
            "    if (last_i >= 0 and last_i != i):\n",
            "        print('flush')\n",
            "        result.append(sum_score)\n",
            "        sum_score = 0\n",
            "    print(i)\n",
            "    sum_score += score[i][j].item()\n",
            "    last_i = i.item()\n",
            "result.append(sum_score)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 139,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "[2.821585178375244, -20.840754985809326]"
                  ]
               },
               "execution_count": 139,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "result"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 36,
         "metadata": {},
         "outputs": [],
         "source": [
            "exp_logit = torch.exp(logit)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 38,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "tensor([-127098.0000, -141032.8750, -140960.9844, -145216.1875, -131136.9375,\n",
                     "        -124833.8438, -140478.4844, -138558.0000, -148117.0469, -142772.9219,\n",
                     "        -141909.0938, -144588.3281, -147351.7812, -148185.6562],\n",
                     "       grad_fn=<SumBackward1>)"
                  ]
               },
               "execution_count": 38,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "torch.sum(logit, -1)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "string = \"你好啊\"\n",
            "list(string)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [
            {
               "ename": "ModuleNotFoundError",
               "evalue": "No module named 'transformers'",
               "output_type": "error",
               "traceback": [
                  "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                  "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
                  "\u001b[1;32m/mnt/disk1/Alfred/Rescoring/tools/debug.ipynb Cell 33\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B140.118.127.81/mnt/disk1/Alfred/Rescoring/tools/debug.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m BertTokenizer\n",
                  "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
               ]
            }
         ],
         "source": [
            "from transformers import BertTokenizer"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [
            {
               "ename": "OSError",
               "evalue": "Model name 'fnlp/bart-base-chinese' was not found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). We assumed 'fnlp/bart-base-chinese' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url.",
               "output_type": "error",
               "traceback": [
                  "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                  "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
                  "\u001b[0;32m<ipython-input-5-9839ceb37a1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fnlp/bart-base-chinese'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
                  "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \"\"\"\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    494\u001b[0m                     \u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms3_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m                     \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_files_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m                 )\n\u001b[1;32m    498\u001b[0m             )\n",
                  "\u001b[0;31mOSError\u001b[0m: Model name 'fnlp/bart-base-chinese' was not found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). We assumed 'fnlp/bart-base-chinese' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url."
               ]
            }
         ],
         "source": [
            "tokenizer = BertTokenizer.from_pretrained('fnlp/bart-base-chinese')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "0"
                  ]
               },
               "execution_count": 6,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "tokenizer.convert_tokens_to_ids('[PAD]')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "string = \"00010002\"\n",
            "string = string.lstrip('0')\n",
            "print(string)\n",
            "split_string = [string[max(i - 4, 0) : i] for i in range(len(string), 0 , -4)]\n",
            "split_string[::-1]"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3.7.11",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.7.11"
      },
      "orig_nbformat": 4,
      "vscode": {
         "interpreter": {
            "hash": "7743e58932f71fb5039bd271cfc730582624e31618fca35b681a356ba051cdac"
         }
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
