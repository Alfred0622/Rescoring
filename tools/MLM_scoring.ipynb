{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_name = f'/mnt/disk3/Alfred/Rescoring/data/aishell/dev/pll_data/token_pll_withLM.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbest = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31\n",
      "0.056374518483887776\n"
     ]
    }
   ],
   "source": [
    "with open(json_name, 'r') as f, \\\n",
    "open(f'/mnt/disk3/Alfred/Rescoring/data/aishell/dev/pll_data/withLM/PLL_hyp.trn', 'w') as hyp, \\\n",
    "open(f'/mnt/disk3/Alfred/Rescoring/data/aishell/dev/pll_data/withLM/PLL_ref.trn', 'w') as ref:\n",
    "    data = json.load(f)\n",
    "    num = 0\n",
    "    cers = []\n",
    "    min_cer = 100\n",
    "    best_weight = 0.0\n",
    "    for num in range(100):\n",
    "        weight = num * 0.01\n",
    "        c = 0\n",
    "        s = 0\n",
    "        de = 0\n",
    "        i = 0\n",
    "        for n, d in enumerate(data):\n",
    "            score = torch.tensor(d['score'][:nbest])\n",
    "            pll = torch.tensor(d['pll'][:nbest])\n",
    "\n",
    "            weight_sum = (1 - weight) * score + weight * pll\n",
    "            \n",
    "            mask_index = torch.argmax(weight_sum).item()\n",
    "\n",
    "\n",
    "            c += d['err'][mask_index][0]\n",
    "            s += d['err'][mask_index][1]\n",
    "            de += d['err'][mask_index][2]\n",
    "            i += d['err'][mask_index][3]\n",
    "            ref_text = list(d['ref_text'])\n",
    "            ref.write(f'{\" \".join(ref_text)} (test_{n + 1})\\n')\n",
    "        cer = (s + de + i) / (c + s + de)\n",
    "        cers.append(cer)\n",
    "        if (cer < min_cer):\n",
    "            min_cer = cer\n",
    "            best_weight = weight\n",
    "    print(best_weight)    \n",
    "    print(min_cer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_name = f'/mnt/disk3/Alfred/Rescoring/data/aishell/dev/pll_data/token_pll_noLM.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_name = f'/mnt/disk3/Alfred/Rescoring/data/aishell/test/pll_data/token_pll_withLM.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31\n",
      "0.04669793173306841\n"
     ]
    }
   ],
   "source": [
    "with open(json_name, 'r') as f, \\\n",
    "    open(f'/mnt/disk3/Alfred/Rescoring/data/aishell/dev/pll_data/noLM/PLL_hyp.trn', 'w') as hyp, \\\n",
    "    open(f'/mnt/disk3/Alfred/Rescoring/data/aishell/dev/pll_data/noLM/PLL_ref.trn', 'w') as ref:\n",
    "    data = json.load(f)\n",
    "    weight = best_weight\n",
    "    print(weight)\n",
    "    c = 0\n",
    "    s = 0\n",
    "    de = 0\n",
    "    i = 0\n",
    "    for n, d in enumerate(data):\n",
    "        score = torch.tensor(d['score'][:nbest])\n",
    "        pll = torch.tensor(d['pll'][:nbest])\n",
    "\n",
    "        weight_sum = (1-weight) * score + weight * pll\n",
    "            \n",
    "        mask_index = torch.argmax(weight_sum).item()\n",
    "        max_text = list(d['text'][mask_index])\n",
    "\n",
    "\n",
    "        hyp.write(f'{\" \".join(max_text)} (test_{n + 1})\\n')\n",
    "\n",
    "        c += d['err'][mask_index][0]\n",
    "        s += d['err'][mask_index][1]\n",
    "        de += d['err'][mask_index][2]\n",
    "        i += d['err'][mask_index][3]\n",
    "        ref_text = list(d['ref_text'])\n",
    "        ref.write(f'{\" \".join(ref_text)} (test_{n + 1})\\n')\n",
    "    cer = (s + de + i) / (c + s + de)\n",
    "    print(cer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4888e53144a1008589b15da35c9d452479f8776cb8933be604ac87ba38a9e5be"
  },
  "kernelspec": {
   "display_name": "espnet",
   "language": "python",
   "name": "espnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
