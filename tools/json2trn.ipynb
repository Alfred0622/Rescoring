{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [],
         "source": [
            "import json\n",
            "import jiwer\n",
            "from tqdm import tqdm"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Prepare Data"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 16,
         "metadata": {},
         "outputs": [],
         "source": [
            "import jiwer\n",
            "from tqdm import tqdm\n",
            "\n",
            "data_name = 'aishell'\n",
            "if (data_name in ['csj']):\n",
            "    recog_set = ['train','dev', 'eval1', 'eval2', 'eval3']\n",
            "elif (data_name in ['aishell2']):\n",
            "    recog_set = [ 'train'] #'dev_ios', 'test_mic', 'test_ios', 'test_android',\n",
            "elif (data_name in ['librispeech']):\n",
            "    recog_set = ['dev_clean', 'dev_other', 'test_clean', 'test_other']\n",
            "elif (data_name in ['aishell']):\n",
            "    recog_set = ['train'] #,'dev', 'test'\n",
            "elif (data_name in ['tedlium2']):\n",
            "    recog_set = ['train','dev', 'dev_trim' ,'test']\n",
            "setting = ['withLM', ' noLM']\n",
            "nbest = 50\n",
            "check_dataset = True"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "'tedlium2'"
                  ]
               },
               "execution_count": 5,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "data_name"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "['train', 'dev', 'dev_trim', 'test']"
                  ]
               },
               "execution_count": 6,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "recog_set"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 31,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "recog set:<class 'list'>\n",
                  "recog:train\n",
                  "tedlium2 - train - withLM: OnlyOneDataCount:0\n",
                  "tedlium2 - train - noLM: OnlyOneDataCount:0\n",
                  "recog:dev\n",
                  "tedlium2 - dev - withLM: OnlyOneDataCount:0\n",
                  "tedlium2 - dev - noLM: OnlyOneDataCount:0\n",
                  "recog:dev_trim\n",
                  "tedlium2 - dev_trim - withLM: OnlyOneDataCount:0\n",
                  "tedlium2 - dev_trim - noLM: OnlyOneDataCount:0\n",
                  "recog:test\n",
                  "tedlium2 - test - withLM: OnlyOneDataCount:0\n",
                  "tedlium2 - test - noLM: OnlyOneDataCount:0\n",
                  "tedlium2: All datas are list\n"
               ]
            }
         ],
         "source": [
            "data_name = 'tedlium2'\n",
            "if (data_name in ['csj']):\n",
            "    recog_set = ['train','dev', 'eval1', 'eval2', 'eval3']\n",
            "elif (data_name in ['aishell2']):\n",
            "    recog_set = ['train', 'dev_ios', 'test_mic', 'test_ios', 'test_android']\n",
            "elif (data_name in ['librispeech']):\n",
            "    recog_set = ['train', 'dev_clean', 'dev_other', 'test_clean', 'test_other']\n",
            "elif (data_name in ['aishell']):\n",
            "    recog_set = ['train', 'dev', 'test'] #,'dev', 'test'\n",
            "elif (data_name in ['tedlium2']):\n",
            "    recog_set = ['train','dev', 'dev_trim' ,'test']\n",
            "setting = ['withLM', 'noLM']\n",
            "FLAG = True\n",
            "print(f'recog set:{type(recog_set)}')\n",
            "for recog in recog_set:\n",
            "    print(f'recog:{recog}')\n",
            "    for s  in setting:\n",
            "        with open(f\"/mnt/disk6/Alfred/Rescoring/data/{data_name}/data/{s}/{recog}/data.json\") as f :\n",
            "            data_json = json.load(f)\n",
            "            if isinstance(data_json, dict):\n",
            "                print(f\"/mnt/disk6/Alfred/Rescoring/data/{data_name}/data/{s}/{recog}/data.json is dict\")\n",
            "                FLAG = False\n",
            "            count = 0\n",
            "            for data in data_json:\n",
            "                if (len(data['hyps']) == 1):\n",
            "                    print(data['name'])\n",
            "                    count += 1\n",
            "            \n",
            "            print(f\"{data_name} - {recog} - {s}: OnlyOneDataCount:{count}\")\n",
            "    \n",
            "if (FLAG):\n",
            "    print(f'{data_name}: All datas are list')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# file_type = '.txt'\n",
            "# trn_name = '.trn'\n",
            "# if (data_name in ['tedlium2', 'librispeech'] and check_dataset):\n",
            "#     file_type = \".wrd.txt\"\n",
            "#     trn_name = '.wrd.trn'\n",
            "# print(file_type)\n",
            "\n",
            "for s in setting:\n",
            "    for task in recog_set:\n",
            "        nbest_err = []\n",
            "        nbest_num = []\n",
            "\n",
            "        data_list = []\n",
            "        print(f'{s}:{task}')\n",
            "        print(f'../data/{data_name}/raw/{s}/{task}/data.json')\n",
            "        with open(f'../data/{data_name}/raw/{s}/{task}/data.json', 'r') as f:\n",
            "        # with open(f'/mnt/disk4/Alfred/espnet/egs/aishell/asr1/exp/rescoring/train_pytorch_rescoring_withLM/decode_train_decode_lm_4/data.json', 'r') as f:\n",
            "            json_data = json.load(f)\n",
            "            for k in json_data['utts'].keys():\n",
            "                nbest_num.append(len(json_data['utts'][k]['output']))\n",
            "            print(f'len of best_num:{len(nbest_num)}')\n",
            "        \n",
            "        for k in tqdm(json_data['utts'].keys(), ncols= 100):\n",
            "            hyps = []\n",
            "            errs = []\n",
            "            scores = []\n",
            "            if ('am_score' in json_data['utts'][k]['output'][0].keys()):\n",
            "                am_scores = []\n",
            "            else:\n",
            "                print(f'no am_score:{k}')\n",
            "                am_scores = None\n",
            "            if ('ctc_score' in json_data['utts'][k]['output'][0].keys()):\n",
            "                ctc_scores = []\n",
            "            else:\n",
            "                print(f'no ctc_score:{k}')\n",
            "                ctc_scores = None\n",
            "            \n",
            "            if (data_name in ['librispeech', 'tedlium2']):\n",
            "                ref = json_data['utts'][k]['output'][0][\"text\"]\n",
            "            else:\n",
            "                ref = json_data['utts'][k]['output'][0][\"token\"]\n",
            "            \n",
            "            if ('lm_score' in json_data['utts'][k]['output'][0].keys()):\n",
            "                lm_scores = []\n",
            "            else:\n",
            "                # print(f'no lm_score:{k}')\n",
            "                lm_scores = None\n",
            "            for hyp in (json_data['utts'][k]['output']):\n",
            "                if (data_name in ['librispeech', 'tedlium2']):\n",
            "                    if (\"<eos>\" in hyp['rec_token'] ):\n",
            "                        hyp_token = hyp['rec_token'][:-5]\n",
            "                    else:\n",
            "                        hyp_token = hyp['rec_token'][:]\n",
            "                    hyp_token = hyp_token.replace(\" \", \"\")\n",
            "                    hyp_token = hyp_token.replace(\"▁\", \" \")\n",
            "                else:\n",
            "                    hyp_token = hyp['rec_token'].split() # remove <eos>\n",
            "                    if ('<eos>' in hyp_token):\n",
            "                        hyp_token = hyp_token[:-1]\n",
            "                    hyp_token = \" \".join(hyp_token)\n",
            "                hyps.append(hyp_token.lower())\n",
            "                \n",
            "                if (am_scores is not None):\n",
            "                    am_scores.append(hyp['am_score'])\n",
            "                if (ctc_scores is not None):\n",
            "                    ctc_scores.append(hyp['ctc_score'])\n",
            "                if (lm_scores is not None):\n",
            "                    lm_scores.append(hyp['lm_score'])\n",
            "                \n",
            "                scores.append(hyp['score'])\n",
            "                \n",
            "                measures = jiwer.compute_measures(ref, hyp_token)\n",
            "\n",
            "                errs.append(\n",
            "                    {\n",
            "                        'err': measures['wer'],\n",
            "                        'hit': measures['hits'],\n",
            "                        'sub': measures['substitutions'],\n",
            "                        'del': measures['deletions'],\n",
            "                        'ins': measures['insertions'],\n",
            "                    }\n",
            "                )\n",
            "\n",
            "            if (len(hyps) <= 1):\n",
            "                print(f'name:{k} \\n hyp:{hyps}')\n",
            "            data_list.append(\n",
            "                {\n",
            "                    'name': k,\n",
            "                    'hyps': hyps,\n",
            "                    'am_score': am_scores,\n",
            "                    'ctc_score': ctc_scores,\n",
            "                    \"lm_score\": None if lm_scores is None else lm_scores,\n",
            "                    \"score\": scores,\n",
            "                    'err': errs,\n",
            "                    'ref': ref.lower()\n",
            "                }\n",
            "            )\n",
            "\n",
            "        with open(f\"../data/{data_name}/data/{s}/{task}/data.json\", 'w') as f:\n",
            "            json.dump(data_list, f, ensure_ascii=False, indent = 1)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Combine librispeech 1~9"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from tqdm import tqdm"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "data_list = []\n",
            "nbest_err = []\n",
            "nbest_num = []\n",
            "\n",
            "data_name = \"librispeech\"\n",
            "\n",
            "low_nBest = 0\n",
            "for num in range(1, 9):\n",
            "    with open(f\"/mnt/disk4/Alfred/espnet/egs/librispeech/asr1/exp/train_960_pytorch_rescoring_withLM/decode_train_960_{num}_model.val5.avg.best_decode_pytorch_transformer_lm/data.json\") as f:\n",
            "        json_data = json.load(f)\n",
            "\n",
            "        for k in json_data['utts'].keys():\n",
            "            if (len(json_data['utts'][k]['output']) < 50):\n",
            "                low_nBest += 1\n",
            "            nbest_num.append(len(json_data['utts'][k]['output']))\n",
            "        print(f'len of data:{len(nbest_num)}')\n",
            "\n",
            "        for k in tqdm(json_data['utts'].keys(), ncols= 100):\n",
            "            hyps = []\n",
            "            errs = []\n",
            "            scores = []\n",
            "            if ('am_score' in json_data['utts'][k]['output'][0].keys()):\n",
            "                am_scores = []\n",
            "            else:\n",
            "                am_scores = None\n",
            "            if ('ctc_score' in json_data['utts'][k]['output'][0].keys()):\n",
            "                ctc_scores = []\n",
            "            else:\n",
            "                ctc_scores = None\n",
            "            \n",
            "            if (data_name in ['librispeech', 'tedlium2']):\n",
            "                ref = json_data['utts'][k]['output'][0][\"text\"]\n",
            "            else:\n",
            "                ref = json_data['utts'][k]['output'][0][\"token\"]\n",
            "            \n",
            "            if ('lm_score' in json_data['utts'][k]['output'][0].keys()):\n",
            "                lm_scores = []\n",
            "            else:\n",
            "                lm_scores = None\n",
            "            for hyp in (json_data['utts'][k]['output']):\n",
            "                if (data_name in ['librispeech', 'tedlium2']):\n",
            "                    if (\"<eos>\" in hyp['rec_token'] ):\n",
            "                        hyp_token = hyp['rec_token'][1:-5]\n",
            "                    else:\n",
            "                        hyp_token = hyp['rec_token'][1:]\n",
            "                    hyp_token = hyp_token.replace(\" \", \"\")\n",
            "                    hyp_token = hyp_token.replace(\"▁\", \" \")\n",
            "                else:\n",
            "                    hyp_token = hyp['rec_token'].split() # remove <eos>\n",
            "                    if ('<eos>' in hyp_token):\n",
            "                        hyp_token = hyp_token[:-1]\n",
            "                    hyp_token = \" \".join(hyp_token)\n",
            "                hyps.append(hyp_token.lower())\n",
            "                \n",
            "                if (am_scores is not None):\n",
            "                    am_scores.append(hyp['am_score'])\n",
            "                if (ctc_scores is not None):\n",
            "                    ctc_scores.append(hyp['ctc_score'])\n",
            "                if (lm_scores is not None):\n",
            "                    lm_scores.append(hyp['lm_score'])\n",
            "                \n",
            "                scores.append(hyp['score'])\n",
            "                \n",
            "                measures = jiwer.compute_measures(ref, hyp_token)\n",
            "\n",
            "                errs.append(\n",
            "                    {\n",
            "                        'err': measures['wer'],\n",
            "                        'hit': measures['hits'],\n",
            "                        'sub': measures['substitutions'],\n",
            "                        'del': measures['deletions'],\n",
            "                        'ins': measures['insertions'],\n",
            "                    }\n",
            "                )\n",
            "            data_list.append(\n",
            "                {\n",
            "                    'name': k,\n",
            "                    'hyps': hyps,\n",
            "                    'am_score': am_scores,\n",
            "                    'ctc_score': ctc_scores,\n",
            "                    \"lm_score\": None if lm_scores is None else lm_scores,\n",
            "                    \"score\": scores,\n",
            "                    'err': errs,\n",
            "                    'ref': ref.lower()\n",
            "                }\n",
            "            )\n",
            "\n",
            "with open(f\"../data/{data_name}/data/withLM/train/data.json\", 'w') as f:\n",
            "    json.dump(data_list, f, ensure_ascii=False, indent =1)\n",
            "\n",
            "print(f\"low_nBest:{low_nBest}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "for setting in ['withLM']:\n",
            "    with open(f'/mnt/disk6/Alfred/Rescoring/data/librispeech/data/{setting}/dev_clean/data.json') as clean, \\\n",
            "         open(f'/mnt/disk6/Alfred/Rescoring/data/librispeech/data/{setting}/dev_other/data.json') as other:\n",
            "\n",
            "        clean_json = json.load(clean)\n",
            "        other_json = json.load(other)\n",
            "\n",
            "        # print(f'{type(clean_json)}')\n",
            "        # break\n",
            "\n",
            "        concat_json = clean_json + other_json\n",
            "\n",
            "        with open(f'/mnt/disk6/Alfred/Rescoring/data/librispeech/data/{setting}/valid/data.json', 'w') as f:\n",
            "            json.dump(concat_json, f, ensure_ascii=False, indent=2)"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Result trn"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "dataname = 'aishell'\n",
            "name = \"Audio_Aware\"\n",
            "setting = 'noLM'\n",
            "recog_set = ['dev', 'test']\n",
            "nbest = 1\n",
            "best_type = 'bart' \n",
            "training = 'MD'"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# BART"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "for r in recog_set:\n",
            "    json_name = f\"./src/Correction/data/{dataname}/{setting}/{r}/{nbest}{best_type}/correct_data.json\"\n",
            "    with open(json_name, 'r') as f, \\\n",
            "            open(f'./src/Correction/data/{dataname}/{setting}/{r}/{nbest}{best_type}/hyp.trn', 'w') as hyp, \\\n",
            "            open(f'./src/Correction/data/{dataname}/{setting}/{r}/{nbest}{best_type}/ref.trn', 'w') as ref:\n",
            "        j = json.load(f)\n",
            "        for k in j.keys():\n",
            "                # print(j['utts'][k]['output'])\n",
            "            hyp_seq = j[k]['hyp']\n",
            "            hyp.write( f'{hyp_seq} ({k})\\n' )\n",
            "            # hyp.write(hyp_seq + '(' + j[\"utts\"][k][\"utt2spk\"].replace(\"-\", \"_\") + '-' + 'hyp' + i + ')')\n",
            "            ref_seq = j[k]['ref']\n",
            "            ref.write( f'{ref_seq} ({k})\\n' )"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# RescoreBert"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "for task in recog_set:\n",
            "    json_name = f\"./data/{dataname}/{task}/{training}/{setting}/{nbest}best_rescore_data.json\"\n",
            "    with open(json_name, 'r') as f, \\\n",
            "        open(f'./data/{dataname}/{task}/{training}/{setting}/{nbest}best_hyp.trn', 'w') as hyp, \\\n",
            "        open(f'./data/{dataname}/{task}/{training}/{setting}/{nbest}best_ref.trn', 'w') as ref:\n",
            "        j = json.load(f)\n",
            "        for k in j['utts'].keys():\n",
            "                # print(j['utts'][k]['output'])\n",
            "            hyp_seq = j['utts'][k]['output']['rec_token']\n",
            "            hyp.write( f'{hyp_seq} ({k})\\n' )\n",
            "            # hyp.write(hyp_seq + '(' + j[\"utts\"][k][\"utt2spk\"].replace(\"-\", \"_\") + '-' + 'hyp' + i + ')')\n",
            "            ref_seq = j['utts'][k]['output']['text_token']\n",
            "            ref.write( f'{ref_seq} ({k})\\n' )\n",
            "    print(f'sclite -r ./{nbest}best_hyp.trn -h  ./{nbest}best_ref.trn -i rm -o all stdout >  ./{nbest}best_result.txt')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "for r in recog_set:\n",
            "    json_name = f'./data/{dataset}_{r}/{name}/rescore_data.json'\n",
            "    with open(json_name, 'r') as f, open(f'./data/{dataset}_{r}/{name}/hyp_rescore.trn', 'w') as hyp, open(f'./data/{dataset}_{r}/{name}/ref_rescore.trn', 'w') as ref:\n",
            "        j = json.load(f)\n",
            "        for k in j['utts'].keys():\n",
            "                # print(j['utts'][k]['output'])\n",
            "            hyp_seq = j['utts'][k]['rec_text']\n",
            "            hyp.write( f'{hyp_seq} ({k})\\n' )\n",
            "            # hyp.write(hyp_seq + '(' + j[\"utts\"][k][\"utt2spk\"].replace(\"-\", \"_\") + '-' + 'hyp' + i + ')')\n",
            "            ref_seq = j['utts'][k]['ref_text']\n",
            "            ref.write( f'{ref_seq} ({k})\\n' )"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from transformers import BertTokenizer\n",
            "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import json\n",
            "import torch\n",
            "# for r in recog_set:\n",
            "best_weight = 0\n",
            "best_cer = 100\n",
            "with open(f\"./data/{dataname}/dev/{nbest}best_{training}_rescore_data_{setting}.json\") as f:\n",
            "    data = json.load(f)\n",
            "    \n",
            "    for n in range(101):\n",
            "        weight = n * 0.01\n",
            "        c = 0\n",
            "        s = 0\n",
            "        de = 0\n",
            "        i = 0\n",
            "        for d in data:\n",
            "            score = torch.tensor(d['score'])\n",
            "            pll = torch.tensor(d['pll'])\n",
            "            cer = d['err']\n",
            "                \n",
            "            result = score + weight * pll\n",
            "\n",
            "            max_index = torch.argmax(result).item()\n",
            "\n",
            "            c += cer[max_index][0]\n",
            "            s += cer[max_index][1]\n",
            "            de += cer[max_index][2]\n",
            "            i += cer[max_index][3]\n",
            " \n",
            "        cer = (s + de + i) / (c + s + de)\n",
            "\n",
            "        if (cer < best_cer):\n",
            "            best_cer = cer\n",
            "            best_weight = weight\n",
            "\n",
            "print(f'best weight:{best_weight} with cer:{best_cer}')\n",
            "\n",
            "for task in recog_set:\n",
            "    with open(f\"./data/{dataname}/{task}/{nbest}best_{training}_rescore_data_{setting}.json\", 'r') as f, \\\n",
            "         open(f'./data/{dataname}/{task}/hyp_mlm.trn', 'w') as h,\\\n",
            "         open(f'./data/{dataname}/{task}/ref_mlm.trn', 'w') as g:\n",
            "        data = json.load(f)\n",
            "        for n, d in enumerate(data):\n",
            "            score = torch.tensor(d['score'])\n",
            "            pll = torch.tensor(d['pll'])\n",
            "            cer = d['err']\n",
            "            ref = d['ref'][5:-5]\n",
            "            token = d['token']\n",
            "                \n",
            "            result = score + weight * pll\n",
            "\n",
            "            max_index = torch.argmax(result).item()\n",
            "\n",
            "            best_hyp = token[max_index]\n",
            "            sep = best_hyp.index(102)\n",
            "            hyp_seq = tokenizer.convert_ids_to_tokens(best_hyp[1:sep])\n",
            "\n",
            "            h.write( f'{\" \".join(hyp_seq)} ({r}_{n})\\n' )\n",
            "            g.write( f'{\" \".join(list(ref))} ({r}_{n})\\n')\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "espnet",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.7.12"
      },
      "orig_nbformat": 4,
      "vscode": {
         "interpreter": {
            "hash": "d61efa0292f535b61ed87be00e22440d3452b09a848af9d1a74444b34776c7f4"
         }
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
