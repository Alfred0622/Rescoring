{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.Datasets import prepareListwiseDataset\n",
    "\n",
    "from utils.CollateFunc import NBestSampler, BatchSampler, crossNBestBatch\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PBERT Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.PrepareModel import prepareNBestCrossBert, preparePBert\n",
    "from bertviz import model_view, head_view\n",
    "import os\n",
    "from jiwer import visualize_alignment, process_characters\n",
    "from utils.LoadConfig import load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PBERT_checkpoint_path = \"/work/jason90255/Rescoring/src/RescoreBert/checkpoint/aishell/NBestCrossBert/noLM/PBERT/50best/RescoreBert_PBERT_batch256_lr1e-7_Freeze-1_HardLabel_Entropy/checkpoint_train_best_CER.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args, train_args, _ = load_config(\"/work/jason90255/Rescoring/src/RescoreBert/config/PBert.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# os.environ['CUDA_VISIBLE_DEVICES']='3'\n",
    "device = torch.device(\"cuda\")\n",
    "PBert_model, tokenizer = preparePBert(\n",
    "    args,\n",
    "    train_args,\n",
    "    device\n",
    ")\n",
    "PBert_model = PBert_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(PBERT_checkpoint_path)\n",
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PBert_model.load_state_dict(torch.load(PBERT_checkpoint_path)['model'])\n",
    "# checkpoint.keys()\n",
    "PBert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_1 = \"\".join(\"但 因 为 聚 集 了 过 多 公 共 思 源\".split())\n",
    "hyp_2 = \"\".join(\"但 因 为 聚 集 了 过 多 公 四 元\".split())\n",
    "hyp_3 = \"但 因 为 聚 集 了 过 多 公 共 思 员\"\n",
    "ref = \"\".join(\"但 因 为 聚 集 了 过 多 公 共 资 源\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_1 = \"但 因 为 聚 集 了 过 多 公 共 思 源\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = process_characters(\n",
    "    [ref, ref],\n",
    "    [hyp_1, hyp_2],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(visualize_alignment(out)))\n",
    "result = visualize_alignment(out, show_measures=False, skip_correct=False).split('\\n')\n",
    "label_align = []\n",
    "\n",
    "for i, r in enumerate(result):\n",
    "    if (i % 5 == 3):\n",
    "        label_sequence = r[5:]\n",
    "        labels = {\n",
    "            \"insertion\": [],\n",
    "            \"deletion\": [],\n",
    "            \"substitution\": []\n",
    "        }\n",
    "        for index, label in enumerate(label_sequence):\n",
    "            if (label == 'S'):\n",
    "                labels['substitution'].append(index)\n",
    "            elif (label == 'D'):\n",
    "                labels['deletion'].append(index)\n",
    "            elif (label == 'I'):\n",
    "                labels['insertion'].append([index, index + 1])\n",
    "        \n",
    "        label_align.append(labels)\n",
    "\n",
    "label_align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"HYP: 但因为聚集了过多公四元*\"[6:])\n",
    "\"HYP: 但因为聚集了过多公四元*\"[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_ids = tokenizer.encode(hyp_1, return_tensors='pt')\n",
    "\n",
    "output = PBert_model.bert(\n",
    "    input_ids = hyp_ids,\n",
    "    output_attentions = True\n",
    ")\n",
    "\n",
    "# attention = output.attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = 0\n",
    "attend_sub = 0\n",
    "attend_del = 0\n",
    "attend_ins = 0\n",
    "attn_weight = output.attentions[-1].sum(dim = 1)[0][0][1:-1] / 12 > 0.05\n",
    "for i, weight in enumerate(attn_weight):\n",
    "    if (weight):\n",
    "        attention += 1\n",
    "        if (i in labels['substitution']):\n",
    "            attend_sub += 1\n",
    "        elif (i in labels['deletion']):\n",
    "            attend_del += 1\n",
    "        elif ([i, i + 1] in labels['inserion']):\n",
    "            attend_ins += 1\n",
    "\n",
    "print(f\"attention weight over threshold:{attention}\")\n",
    "print(f\"attend_sub:{attend_sub}\")\n",
    "print(f\"attend_del:{attend_del}\")\n",
    "print(f\"attend_ins:{attend_ins}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = ['train', 'dev', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/work/jason90255/Rescoring/data/aishell/data/noLM/dev/data.json\") as f:\n",
    "    data_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyps = data_json[0]['hyps']\n",
    "hyps\n",
    "tokenizer.batch_encode_plus(hyps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = 0\n",
    "attend_del = 0\n",
    "attend_ins = 0\n",
    "attend_sub = 0\n",
    "total_character_count = 0\n",
    "for data in tqdm(data_json):\n",
    "    label_dict = {\n",
    "            \"insertion\": [],\n",
    "            \"deletion\": [],\n",
    "            \"substitution\": []\n",
    "        }\n",
    "    process_ref = \"\".join(data['ref'].strip().split())\n",
    "    hyps = [\"\".join(hyp.strip().split()) for hyp in data[\"hyps\"]]\n",
    "    refs = [\"\".join(data['ref'].strip().split()) for _ in range(len(hyps))]\n",
    "    out = process_characters(\n",
    "            refs,\n",
    "            hyps\n",
    "        )\n",
    "    result = visualize_alignment(out, show_measures=False, skip_correct=False).split('\\n')        \n",
    "    for i, r in enumerate(result):\n",
    "        if (i % 5 == 3):\n",
    "            label_sequence = r[5:] \n",
    "            for index, label in enumerate(label_sequence):\n",
    "                if (label == 'S'):\n",
    "                    label_dict['substitution'].append(index)\n",
    "                elif (label == 'I'):\n",
    "                    label_dict['insertion'].append(index)\n",
    "                elif (label == 'D'):\n",
    "                    label_dict['deletion'].append([index, index + 1])\n",
    "        \n",
    "        bert_tokens = tokenizer.batch_encode_plus(data['hyps'], return_tensors='pt', padding = True).to(device)\n",
    "        output = PBert_model.bert(\n",
    "            input_ids = bert_tokens['input_ids'],\n",
    "            attention_mask = bert_tokens['attention_mask'],\n",
    "            output_attentions = True\n",
    "        )\n",
    "\n",
    "        print(len(output.attentions))\n",
    "        \n",
    "        last_attention = output.attentions[-1]\n",
    "\n",
    "        print(last_attention.shape)\n",
    "\n",
    "        break\n",
    "\n",
    "#         attn_weight = output.attentions[-1].sum(dim = 1)[0][0][1:-1] / 12 > 0.05 # CLS attention weight > 0.05 except that attentd to CLS and SEP\n",
    "#         for i, weight in enumerate(attn_weight):\n",
    "#             total_character_count += 1\n",
    "#             if (weight):\n",
    "#                 attention += 1\n",
    "#                 if (i in label_dict['substitution']):\n",
    "#                     attend_sub += 1\n",
    "#                 elif (i in label_dict['insertion']):\n",
    "#                     attend_del += 1\n",
    "#                 elif ([i, i + 1] in label_dict['deletion']):\n",
    "#                     attend_ins += 1\n",
    "\n",
    "# print(f\"total_attention:{total_character_count}\")\n",
    "# print(f\"attention weight over threshold:{attention}\")\n",
    "# print(f\"attend_sub:{attend_sub}\")\n",
    "# print(f\"attend_del:{attend_del}\")\n",
    "# print(f\"attend_ins:{attend_ins}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = 0\n",
    "attend_del = 0\n",
    "attend_ins = 0\n",
    "attend_sub = 0\n",
    "total_character_count = 0\n",
    "for data in tqdm(data_json):\n",
    "    process_ref = \"\".join(data['ref'].strip().split())\n",
    "    for hyp in data['hyps']:\n",
    "        process_hyp = \"\".join(hyp.strip().split())\n",
    "\n",
    "        out = process_characters(\n",
    "            [process_ref],\n",
    "            [process_hyp]\n",
    "        )\n",
    "\n",
    "        result = visualize_alignment(out, show_measures=False, skip_correct=False).split('\\n')\n",
    "\n",
    "        label_dict = {\n",
    "            \"insertion\": [],\n",
    "            \"deletion\": [],\n",
    "            \"substitution\": []\n",
    "        }\n",
    "        \n",
    "        for i, r in enumerate(result):\n",
    "            if (i % 5 == 3):\n",
    "                label_sequence = r[5:] \n",
    "                for index, label in enumerate(label_sequence):\n",
    "                    if (label == 'S'):\n",
    "                        label_dict['substitution'].append(index)\n",
    "                    elif (label == 'I'):\n",
    "                        label_dict['insertion'].append(index)\n",
    "                    elif (label == 'D'):\n",
    "                        label_dict['deletion'].append([index, index + 1])\n",
    "        \n",
    "        hyp_ids = tokenizer.encode(hyp, return_tensors='pt').to(device)\n",
    "        output = PBert_model.bert(\n",
    "            input_ids = hyp_ids,\n",
    "            output_attentions = True\n",
    "        )\n",
    "\n",
    "        attn_weight = output.attentions[-1].sum(dim = 1)[0][0][1:-1] / 12 > 0.05 # CLS attention weight > 0.05 except that attentd to CLS and SEP\n",
    "        for i, weight in enumerate(attn_weight):\n",
    "            total_character_count += 1\n",
    "            if (weight):\n",
    "                attention += 1\n",
    "                if (i in label_dict['substitution']):\n",
    "                    attend_sub += 1\n",
    "                elif (i in label_dict['insertion']):\n",
    "                    attend_del += 1\n",
    "                elif ([i, i + 1] in label_dict['deletion']):\n",
    "                    attend_ins += 1\n",
    "\n",
    "print(f\"total_attention:{total_character_count}\")\n",
    "print(f\"attention weight over threshold:{attention}\")\n",
    "print(f\"attend_sub:{attend_sub}\")\n",
    "print(f\"attend_del:{attend_del}\")\n",
    "print(f\"attend_ins:{attend_ins}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attend_sub / attention\n",
    "attend_del / attention\n",
    "(attend_sub + attend_del + attend_ins) / attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBest Bert Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBestBert = \"/mnt/disk6/Alfred/Rescoring/src/RescoreBert/checkpoint/aishell/NBestCrossBert/noLM/Normal_lstm_KL_sortByLength_dropout0.3_seed42/50best/batch256_lr1e-7_freeze-1/checkpoint_train_best_CER.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(NBestBert)\n",
    "checkpoint['model'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "device = torch.device(\"cpu\")\n",
    "NBest_model, tokenizer = prepareNBestCrossBert(\n",
    "    'aishell',\n",
    "    device,\n",
    "    lstm_dim = 1024,\n",
    "    useNbestCross = False,\n",
    "    lossType = 'KL',\n",
    "    concatCLS = False\n",
    ")\n",
    "NBest_model.load_state_dict(checkpoint['model'])\n",
    "NBest_model = NBest_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_1 = \"但 因 为 聚 集 了 过 多 公 共 思 源\"\n",
    "hyp_2 = \"但 因 为 聚 集 了 过 多 公 共 四 元\"\n",
    "hyp_3 = \"但 因 为 聚 集 了 过 多 公 共 思 员\"\n",
    "ref = \"但 因 为 聚 集 了 过 多 公 共 资 源\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_1 = \"二 人 一 直 先 少 回 应\"\n",
    "hyp_2 = \"二 零 一 直 先 少 回 应\"\n",
    "hyp_3 = \"二 零 一 直 鲜 少 回 应\"\n",
    "ref = \"二 人 一 直 鲜 少 回 应\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_1 = \"北 京 申 办 冬 奥 影 响 远 超 申 办 本 身\" # PBert\n",
    "ref = \"北 京 申 办 冬 奥 影 响 远 超 承 办 本 身\" # NBestBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_1 = \"在 世 锦 赛 决 赛 减 路 前 突 感 不 适\" # NBestBert\n",
    "ref = \"在 世 锦 赛 决 赛 检 录 前 突 感 不 适\" # PBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyps = [hyp_2,  ref]\n",
    "index = 0\n",
    "print(hyps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBestBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyp_ids = tokenizer.encode(hyps[index], return_tensors='pt')\n",
    "\n",
    "output = PBert_model.bert(\n",
    "    input_ids = hyp_ids,\n",
    "    output_attentions = True\n",
    ")\n",
    "attention = output.attentions\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(hyp_ids[0].tolist())\n",
    "single_attention = [att for att in attention]\n",
    "print(single_attention[-1].shape)\n",
    "print(\" \".join(tokens))\n",
    "head_view(attention = single_attention, tokens = tokens) #, html_action='return')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_ids = tokenizer.encode(hyps[index], return_tensors='pt')\n",
    "\n",
    "output = NBest_model.bert(\n",
    "    input_ids = hyp_ids,\n",
    "    output_attentions = True\n",
    ")\n",
    "attention = output.attentions\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(hyp_ids[0].tolist())\n",
    "single_attention = [att for att in attention]\n",
    "print(single_attention[-1].shape)\n",
    "print(\" \".join(tokens))\n",
    "head_view(attention = single_attention, tokens = tokens) #, html_action='return')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_ids = tokenizer.encode(hyps[index], return_tensors='pt')\n",
    "\n",
    "output = model.bert(\n",
    "    input_ids = hyp_ids,\n",
    "    output_attentions = True\n",
    ")\n",
    "attention = output.attentions\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(hyp_ids[0].tolist())\n",
    "single_attention = [att for att in attention]\n",
    "print(single_attention[-1].shape)\n",
    "print(\" \".join(tokens))\n",
    "head_view(attention = single_attention, tokens = tokens) #, html_action='return')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "raw_model = BertModel.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_ids = tokenizer.encode(hyps[index], return_tensors='pt')\n",
    "\n",
    "output = raw_model(\n",
    "    input_ids = hyp_ids,\n",
    "    output_attentions = True\n",
    ")\n",
    "attention = output.attentions\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(hyp_ids[0].tolist())\n",
    "single_attention = [att for att in attention]\n",
    "print(single_attention[-1].shape)\n",
    "print(\" \".join(tokens))\n",
    "head_view(attention = single_attention, tokens = tokens) #, html_action='return')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MaskEMbedBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MaskBertcheckpoint = \"/mnt/disk6/Alfred/Rescoring/src/RescoreBert/checkpoint/aishell/NBestCrossBert/noLM/Normal_query_KL_sortByLength_concatMask_dropout0.3_seed42/50best/batch256_lr1e-7_freeze-1/checkpoint_train_best_CER.pt\"\n",
    "MaskAfterBertcheckpoint = \"/mnt/disk6/Alfred/Rescoring/src/RescoreBert/checkpoint/aishell/NBestCrossBert/noLM/Normal_query_KL_sortByLength_concatMaskAfter_dropout0.3_seed42/50best/batch256_lr1e-7_freeze-1/checkpoint_train_best_CER.pt\"\n",
    "ConcatMaskCheckpoint = \"/mnt/disk6/Alfred/Rescoring/src/RescoreBert/checkpoint/aishell/NBestCrossBert/noLM/Normal_query_KL_sortByLength_concatCLS_concatMaskAfter_dropout0.3_seed42/50best/batch256_lr1e-7_freeze-1/checkpoint_train_best_CER.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MaskBert, tokenizer = prepareNBestCrossBert(\n",
    "    dataset = 'aishell',\n",
    "    device = device, \n",
    "    fuseType = 'query',\n",
    "    concatCLS = False\n",
    ")\n",
    "\n",
    "MaskAfterBert, tokenizer = prepareNBestCrossBert(\n",
    "    dataset = 'aishell',\n",
    "    device = device, \n",
    "    fuseType = 'query',\n",
    "    concatCLS = False\n",
    ")\n",
    "\n",
    "MaskConcatBert, tokenizer = prepareNBestCrossBert(\n",
    "    dataset = 'aishell',\n",
    "    device = device, \n",
    "    fuseType = 'query',\n",
    "    concatCLS = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MaskBert.load_state_dict(torch.load(MaskBertcheckpoint)['model'])\n",
    "MaskAfterBert.load_state_dict(torch.load(MaskAfterBertcheckpoint)['model'])\n",
    "MaskConcatBert.load_state_dict(torch.load(ConcatMaskCheckpoint)['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyp_1 = \"但 因 为 聚 集 了 过 多 公 共 思 源\"\n",
    "hyp_2 = \"但 因 为 聚 集 了 过 多 公 共 四 元\"\n",
    "hyp_3 = \"但 因 为 聚 集 了 过 多 公 共 思 员\"\n",
    "ref = \"但 因 为 聚 集 了 过 多 公 共 资 源\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyp_1_mask = hyp_1 + \"[MASK]\"\n",
    "hyp_2_mask = hyp_2 + \"[MASK]\"\n",
    "hyp_3_mask = hyp_3 + \"[MASK]\"\n",
    "ref_mask = ref + \"[MASK]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyps = [hyp_1, hyp_2, hyp_3, ref]\n",
    "index = 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MaskAfterBert`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyp_ids = tokenizer.encode(hyps[index], return_tensors='pt')\n",
    "mask = tokenizer.convert_tokens_to_ids([\"[MASK]\"])\n",
    "mask = torch.tensor(mask).unsqueeze(0)\n",
    "hyp_ids = torch.cat([hyp_ids, mask], dim = -1)\n",
    "\n",
    "output = MaskAfterBert.bert(\n",
    "    input_ids = hyp_ids,\n",
    "    output_attentions = True\n",
    ")\n",
    "attention = output.attentions\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(hyp_ids[0].tolist())\n",
    "single_attention = [att for att in attention]\n",
    "print(single_attention[-1].shape)\n",
    "print(\" \".join(tokens))\n",
    "head_view(attention = single_attention, tokens = tokens) #, html_action='return')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MaskBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyps_mask = [hyp_1_mask, hyp_2_mask, hyp_3_mask, ref_mask]\n",
    "index = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyp_ids = tokenizer.encode(hyps_mask[index], return_tensors='pt')\n",
    "\n",
    "output = MaskAfterBert.bert(\n",
    "    input_ids = hyp_ids,\n",
    "    output_attentions = True\n",
    ")\n",
    "attention = output.attentions\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(hyp_ids[0].tolist())\n",
    "single_attention = [att for att in attention]\n",
    "print(single_attention[-1].shape)\n",
    "print(\" \".join(tokens))\n",
    "head_view(attention = single_attention, tokens = tokens) #, html_action='return')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask Concat Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyps_mask = [hyp_1_mask, hyp_2_mask, hyp_3_mask, ref_mask]\n",
    "index = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyp_ids = tokenizer.encode(hyps_mask[index], return_tensors='pt')\n",
    "\n",
    "output = MaskConcatBert.bert(\n",
    "    input_ids = hyp_ids,\n",
    "    output_attentions = True\n",
    ")\n",
    "attention = output.attentions\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(hyp_ids[0].tolist())\n",
    "single_attention = [att for att in attention]\n",
    "print(single_attention[-1].shape)\n",
    "print(\" \".join(tokens))\n",
    "head_view(attention = single_attention, tokens = tokens) #, html_action='return')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PBertJson = \"/mnt/disk6/Alfred/Rescoring/data/result/aishell/noLM/test/NBestCrossBert_PBERT_result.json\"\n",
    "NBestCrossJson = \"/mnt/disk6/Alfred/Rescoring/data/result/aishell/noLM/test/NBestCrossBert_lstm_KL_freeze-1_BestCER_result.json\"\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(PBertJson) as P, open(NBestCrossJson) as N:\n",
    "    P_result = json.load(P)\n",
    "    N_result = json.load(N)\n",
    "    \n",
    "    \n",
    "    for p, n in zip(P_result, N_result):\n",
    "        if (p['check_1'] == 'Error' and n['check_1'] == 'Correct'):\n",
    "            print(f\"sit 1 :\\npBert:{p['rescore_hyps']}\\nNBestBert:{n['rescore_hyps']}\")\n",
    "        elif (p['check_1'] == 'Correct' and n['check_1'] == 'Error'):\n",
    "            print(f\"sit 2 :\\npBert:{p['rescore_hyps']}\\nNBestBert:{n['rescore_hyps']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch10_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
