{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.Datasets import prepareListwiseDataset\n",
    "\n",
    "from utils.CollateFunc import NBestSampler, BatchSampler, crossNBestBatch\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from utils.PrepareModel import prepareNBestCrossBert, preparePBert\n",
    "from bertviz import model_view, head_view\n",
    "import os\n",
    "from jiwer import visualize_alignment, process_characters\n",
    "from utils.LoadConfig import load_config\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "args, train_args, _ = load_config(\"/mnt/disk6/Alfred/Rescoring/src/RescoreBert/config/PBert.yaml\")\n",
    "PBert_model, tokenizer = preparePBert(\n",
    "    args,\n",
    "    train_args,\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/mnt/disk6/Alfred/Rescoring/data/aishell/data/noLM/dev/data.json\") as f:\n",
    "    data_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_json = sample(data_json, 2800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer.batch_encode_plus(sample_json[0]['hyps'], return_tensors = 'pt', padding = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PBERT_checkpoint_path = \"/mnt/disk6/Alfred/Rescoring/src/RescoreBert/checkpoint/aishell/fromTWCC/PBERT_TWCC/checkpoint_train_best_CER.pt\"\n",
    "checkpoint = torch.load(PBERT_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PBert_model.load_state_dict(torch.load(PBERT_checkpoint_path)['model'])\n",
    "# checkpoint.keys()\n",
    "PBert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = 0\n",
    "attend_del = 0\n",
    "attend_ins = 0\n",
    "attend_sub = 0\n",
    "total_character_count = 0\n",
    "\n",
    "total_sub = 0\n",
    "total_ins = 0\n",
    "total_del = 0\n",
    "\n",
    "attend_to_sep = 0\n",
    "correctCount = 0\n",
    "correctAttendToSep = 0\n",
    "total_weight = 0\n",
    "for data in tqdm(sample_json):\n",
    "    label_dict = []\n",
    "    process_ref = \"\".join(data['ref'].strip().split())\n",
    "    hyps = [\"\".join(hyp.strip().split()) for hyp in data[\"hyps\"]]\n",
    "    refs = [\"\".join(data['ref'].strip().split()) for _ in range(len(hyps))]\n",
    "    out = process_characters(\n",
    "            refs,\n",
    "            hyps\n",
    "        )\n",
    "    result = visualize_alignment(out, show_measures=False, skip_correct=False).split('\\n')        \n",
    "    for i, r in enumerate(result):\n",
    "        if (i % 5 == 3):\n",
    "            # print(r)\n",
    "            label_sequence = r[5:]\n",
    "            temp_dict = {\n",
    "            \"insertion\": [],\n",
    "            \"deletion\": [],\n",
    "            \"substitution\": []\n",
    "        }\n",
    "            correctFlag = True\n",
    "            for index, label in enumerate(label_sequence):\n",
    "                if (label == 'S'):\n",
    "                    total_sub += 1\n",
    "                    temp_dict['substitution'].append(index)\n",
    "                    correctFlag = False\n",
    "                elif (label == 'I'):\n",
    "                    total_ins += 1\n",
    "                    temp_dict['insertion'].append(index)\n",
    "                    correctFlag = False\n",
    "                elif (label == 'D'):\n",
    "                    total_del += 1\n",
    "                    temp_dict['deletion'].append([index, index + 1])\n",
    "                    correctFlag = False\n",
    "\n",
    "            if (correctFlag):\n",
    "                correctCount += 1\n",
    "            temp_dict['correct'] = correctFlag\n",
    "            label_dict.append(temp_dict)\n",
    "    # print(f'correct:{correctCount}')\n",
    "\n",
    "    bert_tokens = tokenizer.batch_encode_plus(data['hyps'], return_tensors='pt', padding = True).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = PBert_model.bert(\n",
    "            input_ids = bert_tokens['input_ids'],\n",
    "            attention_mask = bert_tokens['attention_mask'],\n",
    "            output_attentions = True\n",
    "        )\n",
    "\n",
    "    last_attention = output.attentions[-1]\n",
    "    \n",
    "    nBest = last_attention.shape[0]\n",
    "    for attention_mask, attention_map, align_label in zip(bert_tokens['attention_mask'], last_attention.sum(dim = 1), label_dict):\n",
    "        token_index = attention_map[0][attention_mask.bool()][1:] / 12\n",
    "        # print(f'token_index:{token_index.shape}')\n",
    "\n",
    "        # target = torch.argmax(token_index)\n",
    "        # attend_index = (token_index > 0.15).nonzero()\n",
    "\n",
    "        # # print(f'attend_index:{attend_index}')\n",
    "        # total_character_count += token_index.shape[0]\n",
    "        # for target in attend_index:\n",
    "        #     attention += 1\n",
    "        #     # if (align_label['correct']):\n",
    "        #     #     correctCount += 1\n",
    "        #     if (target == token_index.shape[0] - 1):\n",
    "        #         attend_to_sep += 1\n",
    "        #         if (align_label['correct'] and attend_index.shape[0] == 1):\n",
    "        #             correctAttendToSep += 1\n",
    "        #     elif (target in align_label['substitution']):\n",
    "        #         attend_sub += 1\n",
    "        #     elif (target in align_label['insertion']):\n",
    "        #         attend_del += 1\n",
    "        #     elif ([target - 1, target] in align_label['deletion'] or [target, target + 1] in align_label['deletion']):\n",
    "        #         attend_ins += 1\n",
    "\n",
    "        sep_weight = token_index[-1].mean()\n",
    "        total_weight += sep_weight / nBest\n",
    "print(total_weight / len(sample_json))\n",
    "\n",
    "# print(f\"total_character_count:{total_character_count}\")\n",
    "# print(f\"total attention:{attention}\")\n",
    "# print(f\"total_sub:{total_sub}\")\n",
    "# print(f\"attend_sub:{attend_sub}\\n\")\n",
    "# print(f\"total_del:{total_del}\")\n",
    "# print(f\"attend_del:{attend_del}\\n\")\n",
    "# print(f\"total_ins:{total_ins}\")\n",
    "# print(f\"attend_ins:{attend_ins}\\n\")\n",
    "\n",
    "# print(attend_to_sep)\n",
    "# print(correctCount)\n",
    "# print(correctAttendToSep)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = 0\n",
    "attend_del = 0\n",
    "attend_ins = 0\n",
    "attend_sub = 0\n",
    "total_character_count = 0\n",
    "\n",
    "sub_hyp = 0\n",
    "del_hyp = 0\n",
    "ins_hyp = 0\n",
    "\n",
    "total_sub = 0\n",
    "total_ins = 0\n",
    "total_del = 0\n",
    "total_si = 0 # substitution & insertion\n",
    "total_sd = 0 # substitution & deletion\n",
    "total_di = 0 # deletion & insertion\n",
    "total_sdi = 0 # three of them\n",
    "\n",
    "\n",
    "attend_to_sep = 0\n",
    "correctCount = 0\n",
    "correctAttendToSep = 0\n",
    "\n",
    "error_dict = {'S' : 0, 'I': 1, 'D' : 2, 'SDI': 3, 'SD': 4, 'SI': 5, 'DI': 6}\n",
    "not_attend_dict = {'S' : 0, 'I': 0, 'D' : 0, 'SDI': 0, 'SD': 0, 'SI': 0, 'DI': 0, 'correct': 0}\n",
    "error_matrix = torch.zeros((7,3), dtype = torch.int32)\n",
    "\n",
    "for data in tqdm(data_json):\n",
    "    label_dict = []\n",
    "    process_ref = \"\".join(data['ref'].strip().split())\n",
    "    hyps = [\"\".join(hyp.strip().split()) for hyp in data[\"hyps\"]]\n",
    "    refs = [\"\".join(data['ref'].strip().split()) for _ in range(len(hyps))]\n",
    "    out = process_characters(\n",
    "            refs,\n",
    "            hyps\n",
    "        )\n",
    "    result = visualize_alignment(out, show_measures=False, skip_correct=False).split('\\n')        \n",
    "    for i, r in enumerate(result):\n",
    "        if (i % 5 == 3):\n",
    "            # print(r)\n",
    "            label_sequence = r[5:]\n",
    "            temp_dict = {\n",
    "            \"insertion\": [],\n",
    "            \"deletion\": [],\n",
    "            \"substitution\": []\n",
    "        }\n",
    "            correctFlag = 'correct'\n",
    "            sub_flag = False\n",
    "            del_flag = False\n",
    "            ins_flag = False\n",
    "            for index, label in enumerate(label_sequence):\n",
    "                if (label == 'S'):\n",
    "                    total_sub += 1\n",
    "                    temp_dict['substitution'].append(index)\n",
    "                    correctFlag = False\n",
    "                    sub_flag = True\n",
    "                elif (label == 'I'):\n",
    "                    total_ins += 1\n",
    "                    temp_dict['insertion'].append(index)\n",
    "                    correctFlag = False\n",
    "                    ins_flag = True\n",
    "                elif (label == 'D'):\n",
    "                    total_del += 1\n",
    "                    temp_dict['deletion'].append([index, index + 1])\n",
    "                    correctFlag = False\n",
    "                    del_flag = True\n",
    "\n",
    "            \n",
    "            if (sub_flag): \n",
    "                if (del_flag):\n",
    "                    if (ins_flag): # S + D + I\n",
    "                        total_sdi += 1\n",
    "                        correctFlag = 'SDI'\n",
    "                    else: # S + D\n",
    "                        total_sd += 1\n",
    "                        correctFlag = 'SD'\n",
    "                else:\n",
    "                    if (ins_flag): # S + I\n",
    "                        total_si += 1\n",
    "                        correctFlag = 'SI'\n",
    "                    else: # S\n",
    "                        sub_hyp += 1\n",
    "                        correctFlag = 'S'\n",
    "            else:\n",
    "                if (del_flag): \n",
    "                    if (ins_flag): # D + I\n",
    "                        total_di += 1\n",
    "                        correctFlag = 'DI'\n",
    "                    else: # D\n",
    "                        del_hyp += 1\n",
    "                        correctFlag = 'D'\n",
    "                else:\n",
    "                    if (ins_flag): # I\n",
    "                        ins_hyp += 1\n",
    "                        correctFlag = 'I'\n",
    "                    else: # None\n",
    "                        correctCount += 1\n",
    "            \n",
    "            temp_dict['correct'] = correctFlag\n",
    "            label_dict.append(temp_dict)\n",
    "    # print(f'correct:{correctCount}')\n",
    "\n",
    "    bert_tokens = tokenizer.batch_encode_plus(data['hyps'], return_tensors='pt', padding = True).to(device)\n",
    "    output = PBert_model.bert(\n",
    "        input_ids = bert_tokens['input_ids'],\n",
    "        attention_mask = bert_tokens['attention_mask'],\n",
    "        output_attentions = True\n",
    "    )\n",
    "\n",
    "    last_attention = output.attentions[-1]\n",
    "\n",
    "    for attention_mask, attention_map, align_label in zip(bert_tokens['attention_mask'], last_attention.sum(dim = 1), label_dict):\n",
    "        token_index = attention_map[0][attention_mask.bool()][1:-1] / 12\n",
    "\n",
    "        target = torch.argmax(token_index)\n",
    "\n",
    "        total_character_count += token_index.shape[0]\n",
    "        attention += 1\n",
    "        # if (target == token_index.shape[0] - 1):\n",
    "        #     attend_to_sep += 1\n",
    "        #     if (align_label['correct'] and attend_index.shape[0] == 1):\n",
    "        #         correctAttendToSep += 1\n",
    "        if (target in align_label['substitution']):\n",
    "            attend_sub += 1\n",
    "            error_matrix[error_dict[align_label['correct']]][error_dict['S']] += 1\n",
    "        elif (target in align_label['insertion']):\n",
    "            attend_ins += 1\n",
    "            error_matrix[error_dict[align_label['correct']]][error_dict['I']] += 1\n",
    "        elif ([target - 1, target] in align_label['deletion'] or [target, target + 1] in align_label['deletion']):\n",
    "            attend_del += 1\n",
    "            error_matrix[error_dict[align_label['correct']]][error_dict['D']] += 1\n",
    "        else: # not attend\n",
    "            not_attend_dict[align_label['correct']] += 1\n",
    "\n",
    "        \n",
    "\n",
    "print(f\"total_character_count:{total_character_count}\")\n",
    "print(f\"total attention:{attention}\")\n",
    "print(f\"total_sub:{total_sub}\")\n",
    "print(f\"attend_sub:{attend_sub}\\n\")\n",
    "print(f\"total_del:{total_del}\")\n",
    "print(f\"attend_del:{attend_del}\\n\")\n",
    "print(f\"total_ins:{total_ins}\")\n",
    "print(f\"attend_ins:{attend_ins}\\n\")\n",
    "\n",
    "print(attend_to_sep)\n",
    "print(correctCount)\n",
    "print(correctAttendToSep)\n",
    "\n",
    "print(error_matrix)\n",
    "print(f'not Attend:\\n {not_attend_dict}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Over threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = 0\n",
    "attend_del = 0\n",
    "attend_ins = 0\n",
    "attend_sub = 0\n",
    "total_character_count = 0\n",
    "\n",
    "sub_hyp = 0\n",
    "del_hyp = 0\n",
    "ins_hyp = 0\n",
    "\n",
    "total_sub = 0\n",
    "total_ins = 0\n",
    "total_del = 0\n",
    "total_si = 0 # substitution & insertion\n",
    "total_sd = 0 # substitution & deletion\n",
    "total_di = 0 # deletion & insertion\n",
    "total_sdi = 0 # three of them\n",
    "\n",
    "\n",
    "attend_to_sep = 0\n",
    "correctCount = 0\n",
    "correctAttendToSep = 0\n",
    "\n",
    "attend_dict = {'S': 0, 'I': 0, 'D': 0, 'N': 0 }\n",
    "missed_dict = {'S': 0, 'I': 0, 'D': 0 } # < threshold 但沒ateend到\n",
    "# error_matrix = torch.zeros((3,3), dtype = torch.int32)\n",
    "\n",
    "for data in tqdm(data_json):\n",
    "    label_dict = []\n",
    "    process_ref = \"\".join(data['ref'].strip().split())\n",
    "    hyps = [\"\".join(hyp.strip().split()) for hyp in data[\"hyps\"]]\n",
    "    refs = [\"\".join(data['ref'].strip().split()) for _ in range(len(hyps))]\n",
    "    out = process_characters(\n",
    "            refs,\n",
    "            hyps\n",
    "        )\n",
    "    result = visualize_alignment(out, show_measures=False, skip_correct=False).split('\\n')        \n",
    "    for i, r in enumerate(result):\n",
    "        if (i % 5 == 3):\n",
    "            # print(r)\n",
    "            label_sequence = r[5:]\n",
    "            temp_dict = {\n",
    "            \"insertion\": [],\n",
    "            \"deletion\": [],\n",
    "            \"substitution\": []\n",
    "        }\n",
    "            correctFlag = True\n",
    "            # sub_flag = False\n",
    "            # del_flag = False\n",
    "            # ins_flag = False\n",
    "            for index, label in enumerate(label_sequence):\n",
    "                if (label == 'S'):\n",
    "                    total_sub += 1\n",
    "                    temp_dict['substitution'].append(index)\n",
    "                    correctFlag = False\n",
    "                    # sub_flag = True\n",
    "                elif (label == 'I'):\n",
    "                    total_ins += 1\n",
    "                    temp_dict['insertion'].append(index)\n",
    "                    correctFlag = False\n",
    "                    # ins_flag = True\n",
    "                elif (label == 'D'):\n",
    "                    total_del += 1\n",
    "                    temp_dict['deletion'].append([index, index + 1])\n",
    "                    correctFlag = False\n",
    "                    # del_flag = True\n",
    "\n",
    "            if (correctFlag):\n",
    "                correctCount += 1\n",
    "            \n",
    "            temp_dict['correct'] = correctFlag\n",
    "            label_dict.append(temp_dict)\n",
    "\n",
    "    bert_tokens = tokenizer.batch_encode_plus(data['hyps'], return_tensors='pt', padding = True).to(device)\n",
    "    output = PBert_model.bert(\n",
    "        input_ids = bert_tokens['input_ids'],\n",
    "        attention_mask = bert_tokens['attention_mask'],\n",
    "        output_attentions = True\n",
    "    )\n",
    "\n",
    "    last_attention = output.attentions[-1]\n",
    "\n",
    "    for attention_mask, attention_map, align_label in zip(bert_tokens['attention_mask'], last_attention.sum(dim = 1), label_dict):\n",
    "        token_index = attention_map[0][attention_mask.bool()][1:-1] / 12\n",
    "        length = attention_mask.shape[0]\n",
    "        threshold = 1 / length\n",
    "\n",
    "        # attend_index = (token_index > 0.2).nonzero()\n",
    "\n",
    "        total_character_count += token_index.shape[0]\n",
    "        for target, attention_weight in enumerate(token_index):\n",
    "            if (attention_weight >= threshold):\n",
    "                attention += 1\n",
    "                if (target in align_label['substitution']):\n",
    "                    attend_dict['S'] += 1\n",
    "                elif (target in align_label['insertion']):\n",
    "                    attend_dict['I'] += 1\n",
    "                elif ([target - 1, target] in align_label['deletion'] or [target, target + 1] in align_label['deletion']):\n",
    "                    attend_dict['D'] += 1\n",
    "                else: # not attend\n",
    "                    attend_dict['N'] += 1\n",
    "            else:\n",
    "                if (target in align_label['substitution']):\n",
    "                    missed_dict['S'] += 1\n",
    "                elif (target in align_label['insertion']):\n",
    "                    missed_dict['I'] += 1\n",
    "                elif ([target - 1, target] in align_label['deletion'] or [target, target + 1] in align_label['deletion']):\n",
    "                    missed_dict['D'] += 1\n",
    "\n",
    "print(f\"total_character_count:{total_character_count}\")\n",
    "print(f\"total attention over threshold:{attention}\")\n",
    "print(f\"total_sub:{total_sub}\")\n",
    "print(f\"total_ins:{total_ins}\")\n",
    "print(f\"total_del:{total_del}\")\n",
    "\n",
    "print(f\"attend dict:\\n{attend_dict}\")\n",
    "\n",
    "print(f\"missed dict:\\n{missed_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'S' : 0, 'I': 1, 'D' : 2, 'SDI': 3, 'SD': 4, 'SI': 5, 'DI': 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'total_sub:{sub_hyp + total_si + total_sd + total_sdi}')\n",
    "print(f'total_sub_ins:{total_si}')\n",
    "print(f'total_sub_del:{total_sd}')\n",
    "print(f'total_sub_del_ins:{total_sdi}')\n",
    "\n",
    "print((attend_sub / (sub_hyp + total_si + total_sd + total_sdi)))\n",
    "\n",
    "print(f'total_del:{del_hyp + total_di + total_sd + total_sdi}')\n",
    "print(attend_del / (del_hyp + total_di + total_sd + total_sdi))\n",
    "\n",
    "print(f'total_ins:{ins_hyp + total_di + total_si + total_sdi}')\n",
    "print(attend_ins / (ins_hyp + total_di + total_si + total_sdi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"total_character_count:{total_character_count}\")\n",
    "print(f\"attention:{attention}\")\n",
    "print(f\"attend_sub:{attend_sub}\")\n",
    "print(f\"attend_del:{attend_del}\")\n",
    "print(f\"attend_ins:{attend_ins}\")\n",
    "\n",
    "print(f\"total_sub:{total_sub}\")\n",
    "print(f\"total_ins:{total_ins}\")\n",
    "print(f\"total_del:{total_del}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = 0\n",
    "attend_del = 0\n",
    "attend_ins = 0\n",
    "attend_sub = 0\n",
    "total_character_count = 0\n",
    "\n",
    "total_sub = 0\n",
    "total_ins = 0\n",
    "total_del = 0\n",
    "\n",
    "attend_to_sep = 0\n",
    "correctCount = 0\n",
    "correctAttendToSep = 0\n",
    "\n",
    "for data in tqdm(data_json):\n",
    "    label_dict = []\n",
    "    process_ref = \"\".join(data['ref'].strip().split())\n",
    "    hyps = [\"\".join(hyp.strip().split()) for hyp in data[\"hyps\"]]\n",
    "    refs = [\"\".join(data['ref'].strip().split()) for _ in range(len(hyps))]\n",
    "    out = process_characters(\n",
    "            refs,\n",
    "            hyps\n",
    "        )\n",
    "    result = visualize_alignment(out, show_measures=False, skip_correct=False).split('\\n')        \n",
    "    for i, r in enumerate(result):\n",
    "        if (i % 5 == 3):\n",
    "            # print(r)\n",
    "            label_sequence = r[5:]\n",
    "            temp_dict = {\n",
    "            \"insertion\": [],\n",
    "            \"deletion\": [],\n",
    "            \"substitution\": []\n",
    "        }\n",
    "            correctFlag = True\n",
    "            for index, label in enumerate(label_sequence):\n",
    "                if (label == 'S'):\n",
    "                    total_sub += 1\n",
    "                    temp_dict['substitution'].append(index)\n",
    "                    correctFlag = False\n",
    "                elif (label == 'I'):\n",
    "                    total_ins += 1\n",
    "                    temp_dict['insertion'].append(index)\n",
    "                    correctFlag = False\n",
    "                elif (label == 'D'):\n",
    "                    total_del += 1\n",
    "                    temp_dict['deletion'].append([index, index + 1])\n",
    "                    correctFlag = False\n",
    "\n",
    "            if (correctFlag):\n",
    "                correctCount += 1\n",
    "            temp_dict['correct'] = correctFlag\n",
    "            label_dict.append(temp_dict)\n",
    "    # print(f'correct:{correctCount}')\n",
    "\n",
    "    bert_tokens = tokenizer.batch_encode_plus(data['hyps'], return_tensors='pt', padding = True).to(device)\n",
    "    output = PBert_model.bert(\n",
    "        input_ids = bert_tokens['input_ids'],\n",
    "        attention_mask = bert_tokens['attention_mask'],\n",
    "        output_attentions = True\n",
    "    )\n",
    "\n",
    "    last_attention = output.attentions[-1]\n",
    "\n",
    "    for attention_mask, attention_map, align_label in zip(bert_tokens['attention_mask'], last_attention.sum(dim = 1), label_dict):\n",
    "        token_index = attention_map[0][attention_mask.bool()][1:] / 12\n",
    "        # print(f'token_index:{token_index.shape}')\n",
    "\n",
    "        target = torch.argmax(token_index)\n",
    "        # attend_index = (token_index > 0.2).nonzero()\n",
    "\n",
    "        # print(f'attend_index:{attend_index}')\n",
    "        total_character_count += token_index.shape[0]\n",
    "        # for target in attend_index:\n",
    "        attention += 1\n",
    "        # if (align_label['correct']):\n",
    "        #     correctCount += 1\n",
    "        if (target == token_index.shape[0] - 1):\n",
    "            attend_to_sep += 1\n",
    "            if (align_label['correct']): # and attend_index.shape[0] == 1):\n",
    "                correctAttendToSep += 1\n",
    "        elif (target in align_label['substitution']):\n",
    "            attend_sub += 1\n",
    "        elif (target in align_label['insertion']):\n",
    "            attend_del += 1\n",
    "        elif ([target - 1, target] in align_label['deletion'] or [target, target + 1] in align_label['deletion']):\n",
    "            attend_ins += 1\n",
    "\n",
    "print(f\"total_character_count:{total_character_count}\")\n",
    "print(f\"total attention:{attention}\")\n",
    "print(f\"total_sub:{total_sub}\")\n",
    "print(f\"attend_sub:{attend_sub}\\n\")\n",
    "print(f\"total_del:{total_del}\")\n",
    "print(f\"attend_del:{attend_del}\\n\")\n",
    "print(f\"total_ins:{total_ins}\")\n",
    "print(f\"attend_ins:{attend_ins}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(attend_to_sep)\n",
    "print(correctCount)\n",
    "print(correctAttendToSep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch10_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
