{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.Datasets import prepareListwiseDataset\n",
    "\n",
    "from utils.CollateFunc import NBestSampler, BatchSampler, crossNBestBatch\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from utils.PrepareModel import prepareNBestCrossBert, preparePBert\n",
    "from bertviz import model_view, head_view\n",
    "import os\n",
    "from jiwer import visualize_alignment, process_characters\n",
    "from utils.LoadConfig import load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "args, train_args, _ = load_config(\"/work/jason90255/Rescoring/src/RescoreBert/config/PBert.yaml\")\n",
    "PBert_model, tokenizer = preparePBert(\n",
    "    args,\n",
    "    train_args,\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/work/jason90255/Rescoring/data/aishell/data/noLM/dev/data.json\") as f:\n",
    "    data_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer.batch_encode_plus(data_json[0]['hyps'], return_tensors = 'pt', padding = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PBERT_checkpoint_path = \"/work/jason90255/Rescoring/src/RescoreBert/checkpoint/aishell/NBestCrossBert/noLM/PBERT/50best/RescoreBert_PBERT_batch256_lr1e-7_Freeze-1_HardLabel_Entropy/checkpoint_train_best_CER.pt\"\n",
    "checkpoint = torch.load(PBERT_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PBert_model.load_state_dict(torch.load(PBERT_checkpoint_path)['model'])\n",
    "# checkpoint.keys()\n",
    "PBert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d411316ebc084f48869bdf4e02ca6244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14326 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m last_attention \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mattentions[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     45\u001b[0m \u001b[39mfor\u001b[39;00m attention_mask, attention_map, align_label \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(bert_tokens[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m], last_attention\u001b[39m.\u001b[39msum(dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m), label_dict):\n\u001b[0;32m---> 46\u001b[0m     token_index \u001b[39m=\u001b[39m attention_map[\u001b[39m0\u001b[39;49m][attention_mask\u001b[39m.\u001b[39;49mbool()][\u001b[39m1\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m/\u001b[39m \u001b[39m12\u001b[39m\n\u001b[1;32m     47\u001b[0m     attend_index \u001b[39m=\u001b[39m (token_index \u001b[39m>\u001b[39m \u001b[39m0.1\u001b[39m)\u001b[39m.\u001b[39mnonzero()\n\u001b[1;32m     48\u001b[0m     \u001b[39m# print(f'token_index:{token_index.shape}')\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[39m# print(f'attend_index:{attend_index.shape}')\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "attention = 0\n",
    "attend_del = 0\n",
    "attend_ins = 0\n",
    "attend_sub = 0\n",
    "total_character_count = 0\n",
    "\n",
    "total_sub = 0\n",
    "total_ins = 0\n",
    "total_del = 0\n",
    "for data in tqdm(data_json):\n",
    "    label_dict = []\n",
    "    process_ref = \"\".join(data['ref'].strip().split())\n",
    "    hyps = [\"\".join(hyp.strip().split()) for hyp in data[\"hyps\"]]\n",
    "    refs = [\"\".join(data['ref'].strip().split()) for _ in range(len(hyps))]\n",
    "    out = process_characters(\n",
    "            refs,\n",
    "            hyps\n",
    "        )\n",
    "    result = visualize_alignment(out, show_measures=False, skip_correct=False).split('\\n')        \n",
    "    for i, r in enumerate(result):\n",
    "        if (i % 5 == 3):\n",
    "            # print(r)\n",
    "            label_sequence = r[5:]\n",
    "            temp_dict = {\n",
    "            \"insertion\": [],\n",
    "            \"deletion\": [],\n",
    "            \"substitution\": []\n",
    "        }\n",
    "            for index, label in enumerate(label_sequence):\n",
    "                if (label == 'S'):\n",
    "                    temp_dict['substitution'].append(index)\n",
    "                elif (label == 'I'):\n",
    "                    temp_dict['insertion'].append(index)\n",
    "                elif (label == 'D'):\n",
    "                    temp_dict['deletion'].append([index, index + 1])\n",
    "    \n",
    "            label_dict.append(temp_dict)\n",
    "    \n",
    "        \n",
    "    bert_tokens = tokenizer.batch_encode_plus(data['hyps'], return_tensors='pt', padding = True).to(device)\n",
    "    output = PBert_model.bert(\n",
    "        input_ids = bert_tokens['input_ids'],\n",
    "        attention_mask = bert_tokens['attention_mask'],\n",
    "        output_attentions = True\n",
    "    )\n",
    "    \n",
    "    last_attention = output.attentions[-1]\n",
    "\n",
    "    for attention_mask, attention_map, align_label in zip(bert_tokens['attention_mask'], last_attention.sum(dim = 1), label_dict):\n",
    "        token_index = attention_map[0][attention_mask.bool()][1:-1] / 12\n",
    "        attend_index = (token_index > 0.15).nonzero()\n",
    "        # print(f'token_index:{token_index.shape}')\n",
    "        # print(f'attend_index:{attend_index.shape}')\n",
    "        total_character_count += token_index.shape[0]\n",
    "        for target in attend_index:\n",
    "            attention += 1\n",
    "            if (target in align_label['substitution']):\n",
    "                attend_sub += 1\n",
    "            elif (target in align_label['insertion']):\n",
    "                attend_del += 1\n",
    "            elif ([target - 1, target] in align_label['deletion'] or [target, target + 1] in align_label['deletion']):\n",
    "                attend_ins += 1\n",
    "\n",
    "print(f\"total_character_count:{total_character_count}\")\n",
    "print(attention)\n",
    "print(attend_sub)\n",
    "print(attend_del)\n",
    "print(attend_ins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch10_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
